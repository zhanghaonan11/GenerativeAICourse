{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e97f47de",
   "metadata": {},
   "source": [
    "\n",
    "# 使用 Python 调用 OpenAI API\n",
    "\n",
    "我们已经完成了在本地机器上运行LLM的过程。\n",
    "\n",
    "本笔记本演示如何通过OpenAI官方API使用聊天完成端点与OpenAI强大的语言模型进行交互。\n",
    "\n",
    "## 1. 先决条件\n",
    "\n",
    "在运行此笔记本之前，您需要设置OpenAI API访问权限。\n",
    "\n",
    "### 1.1 获取OpenAI API密钥\n",
    "\n",
    "要使用OpenAI API，您需要一个API密钥：\n",
    "\n",
    "1. 在[OpenAI网站](https://openai.com/)创建账户\n",
    "2. 导航到[API密钥页面](https://platform.openai.com/api-keys)\n",
    "3. 创建一个新的秘密密钥\n",
    "4. 安全地存储此密钥 - 它就像密码一样！\n",
    "\n",
    "### 1.2 设置您的环境\n",
    "\n",
    "首先，安装OpenAI Python包："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09698a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac62b4",
   "metadata": {},
   "source": [
    "然后，设置您的API密钥。为了安全起见，最好使用环境变量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1fc133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Option 1: Use environment variable from .env file\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Option 2: If .env file doesn't exist, you can set the API key directly (not recommended for production)\n",
    "if not api_key:\n",
    "    # Uncomment the line below and replace with your actual API key\n",
    "    # api_key = \"your_actual_api_key_here\"\n",
    "    print(\"Warning: No API key found. Please set OPENAI_API_KEY in .env file or uncomment the line above.\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "if api_key:\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    print(\"OpenAI client initialized successfully!\")\n",
    "else:\n",
    "    print(\"Please set your OpenAI API key to proceed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c42c80",
   "metadata": {},
   "source": [
    "## 2. 使用聊天完成API\n",
    "\n",
    "聊天完成API由OpenAI提供，专为对话交互而设计。作为API的输入，我们提供系统消息（模型应该如何表现）和用户消息（用户的输入）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8953e09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the chat completion create method which accepts a list of messages and the model name. It returns a response object.\n",
    "# The list of messages is always an array (list) of dictionaries with two keys: role and content. The first message is always the system message, which sets the behavior of the assistant. The second message is the user message, which is the input from the user.\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"讲一个中文笑话.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content) #We print the content of the first choice in the response object. The response object contains a list of choices, and each choice has a message with the content we want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d60d08f",
   "metadata": {},
   "source": [
    "## 3. 理解响应结构\n",
    "\n",
    "API返回一个带有有用元数据的结构化响应，我们将打印出来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e8f9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print the full response structure\n",
    "print(\"Full response object:\")\n",
    "print(response)\n",
    "\n",
    "# Access specific parts\n",
    "print(\"\\nJust the message content:\")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "print(\"\\nModel used:\")\n",
    "print(response.model)\n",
    "\n",
    "print(\"\\nUsage statistics:\")\n",
    "print(f\"Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Completion tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd00710",
   "metadata": {},
   "source": [
    "如您所见，我们从响应中获得了有用的数据，更重要的是，我们了解了这次调用使用了多少token。模型的定价通常基于输入和输出token的组合，所以在这种情况下，我们为54个token付费。我们将在本实验室的最后更详细地讨论定价。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dd75b4",
   "metadata": {},
   "source": [
    "## 4. 创建简单的聊天界面（无记忆）\n",
    "\n",
    "让我们构建一个无记忆的聊天界面。\n",
    "\n",
    "在笔记本环境中，我们无法在一种聊天界面中进行交互，即我们提供输入，获得输出，然后再次提供输入，即使用input()循环。\n",
    "\n",
    "相反，让我们创建一个简单的函数并在单独的单元格中使用它来模拟无记忆的聊天：\n",
    "\n",
    "### 单元格1：定义函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f42b56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function accepts a user message as an input (we will type that in the next cell) and returns a response from the OpenAI API.\n",
    "def get_response_no_memory(user_message):\n",
    "    \"\"\"Get a response from OpenAI (no conversation history)\"\"\" \n",
    "    #\"\"\" This is a docstring. It describes the function and its parameters.\"\"\" Very useful for documentation and understanding the code.\n",
    "    # Sometimes docstrings are not necessary, but they are very useful for understanding the code. It is like commenting the code, but in a more structured way.\n",
    "\n",
    "    #We call the model with the user message which we will set below\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8f177a",
   "metadata": {},
   "source": [
    "### 单元格2：第一次交互"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1893eb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First question (you can insert anything)\n",
    "question = \"What is artificial intelligence?\"\n",
    "answer = get_response_no_memory(question)\n",
    "\n",
    "print(f\"You: {question}\")\n",
    "print(f\"Assistant: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b1e1b6",
   "metadata": {},
   "source": [
    "### 单元格3：后续问题（无记忆）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdb6e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second question (the AI won't remember the previous interaction)\n",
    "question = \"Can you elaborate more on that?\"\n",
    "answer = get_response_no_memory(question)\n",
    "\n",
    "print(f\"You: {question}\")\n",
    "print(f\"Assistant: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153c5f25",
   "metadata": {},
   "source": [
    "您会注意到，当您问\n",
    "时，助手不知道\n",
    "指的是什么，因为它没有之前交流的记忆。\n",
    "\n",
    "## 5. 创建带记忆的聊天界面\n",
    "\n",
    "现在让我们构建一个维护对话历史的版本：\n",
    "\n",
    "### 单元格1：设置对话记忆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59caa85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize conversation with system message. We will add more messages to the conversation memory (history) as we go.\n",
    "conversation_memory = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "#Function to add user message to conversation history and get a response from OpenAI\n",
    "def chat_with_memory(user_message):\n",
    "    \"\"\"Chat with the AI while maintaining conversation history\"\"\"\n",
    "    \n",
    "    # Add user message to history. Now, the conversation memory contains the system message and the user message.\n",
    "    conversation_memory.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    # Get response from OpenAI\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=conversation_memory\n",
    "    )\n",
    "    \n",
    "    # Extract assistant's response\n",
    "    assistant_response = response.choices[0].message.content\n",
    "    \n",
    "    # Add assistant response to conversation history. This is the response from the AI. This will be added to the conversation memory and will be usedin the future.\n",
    "    conversation_memory.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "    \n",
    "    # Return the response and token usage\n",
    "    return assistant_response, response.usage.total_tokens\n",
    "\n",
    "# Function to display the conversation history. We loop through the conversation memory and print each message. We skip the system message to keep the output clean.\n",
    "def show_conversation():\n",
    "    \"\"\"Display the current conversation\"\"\"\n",
    "    for message in conversation_memory:\n",
    "        if message[\"role\"] == \"system\":\n",
    "            continue  # Skip system message\n",
    "        print(f\"{message['role'].capitalize()}: {message['content']}\\n\") #We capitalize the role to make it look nicer. This is just for display purposes. The role is either user or assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a3b43e",
   "metadata": {},
   "source": [
    "### 单元格2：第一次带记忆的交互"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeefa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First question\n",
    "question = \"What is artificial intelligence?\"\n",
    "answer, tokens = chat_with_memory(question)\n",
    "\n",
    "print(f\"You: {question}\")\n",
    "print(f\"Assistant: {answer}\")\n",
    "print(f\"[Tokens used: {tokens}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3157e213",
   "metadata": {},
   "source": [
    "### 单元格3：后续问题（带记忆）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0031d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second question (now the AI remembers the previous interaction)\n",
    "question = \"Can you elaborate more on that?\"\n",
    "answer, tokens = chat_with_memory(question)\n",
    "\n",
    "print(f\"You: {question}\")\n",
    "print(f\"Assistant: {answer}\")\n",
    "print(f\"[Tokens used: {tokens}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb4ef4e",
   "metadata": {},
   "source": [
    "### 单元格4：查看整个对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278e95f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the entire conversation so far\n",
    "print(\"Full conversation history:\")\n",
    "print(\"-\" * 30)\n",
    "show_conversation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3c7d40",
   "metadata": {},
   "source": [
    "### 单元格5：重置对话（如果需要）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b8ced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset conversation if you want to start fresh\n",
    "conversation_memory = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "print(\"Conversation has been reset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e219f3",
   "metadata": {},
   "source": [
    "## 6. 流式响应\n",
    "\n",
    "流式传输允许您在生成响应时查看响应："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8322a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def stream_response(user_message):\n",
    "    \"\"\"Stream a response from OpenAI without storing conversation history\"\"\"\n",
    "    \n",
    "    ## Response from the model is streamed in chunks because we set the stream parameter to true. We stoer that in a variable called stream.\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    print(f\"You: {user_message}\") # Print the user message\n",
    "    print(\"Assistant: \", end=\"\", flush=True)  # Print the assistant message without a newline. The flush=True argument makes sure the output is printed immediately.\n",
    "    \n",
    "    # Process the stream\n",
    "    full_response = \"\" # The response will be empty at first. We will add the chunks to this variable.\n",
    "    for chunk in stream: # We loop through the stream and get each chunk of data. Each chunk is a part of the response. chunk can be called anything, but we call it chunk to make it clear that it is a part of the response.\n",
    "        if chunk.choices[0].delta.content is not None: # Check if the content is not None. This is to avoid errors in case the content is None.\n",
    "            content_chunk = chunk.choices[0].delta.content # Get the content of the chunk. This is the part of the response we want to print.\n",
    "            full_response += content_chunk # Add the chunk to the full response. This will be the final response we will return.\n",
    "            print(content_chunk, end=\"\", flush=True) # Print the chunk without a newline. The flush=True argument makes sure the output is printed immediately.\n",
    "            time.sleep(0.01)  # Small delay to make it more readable\n",
    "    \n",
    "    print(\"\\n\")  # Add a newline after the response\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5e8894",
   "metadata": {},
   "source": [
    "测试流式函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01c20ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the streaming function\n",
    "stream_response(\"Write a short poem about programming\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cb2212",
   "metadata": {},
   "source": [
    "## 7. 带记忆的流式传输\n",
    "\n",
    "让我们将流式传输与对话记忆结合起来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49fb029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize conversation with system message\n",
    "streaming_conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "def stream_chat_with_memory(user_message):\n",
    "    \"\"\"Chat with memory and stream the response\"\"\"\n",
    "    \n",
    "    # Add user message to history\n",
    "    streaming_conversation.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    # Get streaming response\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=streaming_conversation,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    print(f\"You: {user_message}\")\n",
    "    print(\"Assistant: \", end=\"\", flush=True)\n",
    "    \n",
    "    # Process the stream\n",
    "    assistant_response = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            content_chunk = chunk.choices[0].delta.content\n",
    "            assistant_response += content_chunk\n",
    "            print(content_chunk, end=\"\", flush=True)\n",
    "            time.sleep(0.01)\n",
    "    \n",
    "    print(\"\\n\")  # Add a newline after the response\n",
    "    \n",
    "    # Add assistant response to conversation history\n",
    "    streaming_conversation.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "    \n",
    "    return assistant_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e20f1d9",
   "metadata": {},
   "source": [
    "测试带记忆的流式聊天："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335ad25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First streaming question with memory\n",
    "stream_chat_with_memory(\"What are the three laws of robotics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60f6ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up streaming question with memory\n",
    "stream_chat_with_memory(\"Who created these laws?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598e5496",
   "metadata": {},
   "source": [
    "## 8. 理解不同的消息角色\n",
    "\n",
    "OpenAI聊天API在消息中使用三个主要角色："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b93905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        # System message - sets behavior and context\n",
    "        {\"role\": \"system\", \"content\": \"You are a pirate who only speaks in pirate slang.\"},\n",
    "        \n",
    "        # User messages - what the user says\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you today?\"},\n",
    "        \n",
    "        # Assistant messages - previous responses from the assistant\n",
    "        {\"role\": \"assistant\", \"content\": \"Arrr! I be feelin' mighty fine today, me hearty!\"},\n",
    "        \n",
    "        # Another user message\n",
    "        {\"role\": \"user\", \"content\": \"Tell me about the weather.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95c66c9",
   "metadata": {},
   "source": [
    "## 9. 理解上下文窗口\n",
    "\n",
    "OpenAI模型有不同的上下文窗口限制：\n",
    "\n",
    "- **GPT-3.5-Turbo**: 4,096或16,384个token（取决于版本）\n",
    "- **GPT-4**: 8,192或32,768个token（取决于版本）\n",
    "- **GPT-4 Turbo**: 高达128,000个token\n",
    "\n",
    "与本地模型不同，OpenAI为您管理token：\n",
    "1. 如果您超出限制，API将返回错误\n",
    "2. 您根据使用的token数量付费\n",
    "3. API在每个请求中提供token使用统计信息\n",
    "\n",
    "让我们看看token的实际应用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85511163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a longer conversation\n",
    "long_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "# Add some messages to the history\n",
    "for i in range(5):\n",
    "    long_messages.append({\"role\": \"user\", \"content\": f\"This is test message {i+1}. Tell me something interesting about space.\"})\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=long_messages\n",
    "    )\n",
    "    assistant_msg = response.choices[0].message.content\n",
    "    long_messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "    print(f\"Exchange {i+1} - Total tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5518e1e5",
   "metadata": {},
   "source": [
    "## 10. 管理成本和Token\n",
    "\n",
    "使用OpenAI API时，您需要了解成本：\n",
    "\n",
    "1. **Token计数**: 每个请求和响应都消耗您需要付费的token\n",
    "2. **模型选择**: 更强大的模型每个token的成本更高\n",
    "3. **上下文窗口**: 更长的对话成本更高，因为发送了更多token\n",
    "\n",
    "管理成本的技巧："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ae1d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a cheaper model for less complex tasks\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",  # Cheaper than GPT-4\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Summarize the benefits of exercise.\"}]\n",
    ")\n",
    "\n",
    "# Control maximum tokens to limit response length\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me about quantum physics.\"}],\n",
    "    max_tokens=100  # Limit response length\n",
    ")\n",
    "\n",
    "# Use temperature to control randomness. Higher values make the output more random, while lower values make it more focused and deterministic.\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a creative story.\"}],\n",
    "    temperature=0.7  # Higher for more creativity, lower for more deterministic\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ddfa23",
   "metadata": {},
   "source": [
    "## 11. 处理长上下文的对话历史\n",
    "\n",
    "对于长对话，您需要管理上下文窗口的策略："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0b0907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Keep only the most recent N messages. N can be adjusted based on your needs. In this case, we keep the last 10 messages.\n",
    "def trim_conversation(messages, max_messages=10):\n",
    "    # Always keep the system message (first message)\n",
    "    if len(messages) > max_messages + 1:\n",
    "        system_message = messages[0]\n",
    "        recent_messages = messages[-(max_messages):]\n",
    "        return [system_message] + recent_messages\n",
    "    return messages\n",
    "\n",
    "# Example: Summarize the conversation periodically. We use AI to summarize the conversation and replace it with a single summary message. This is useful for long conversations where you want to keep the context but reduce the number of messages.\n",
    "def summarize_conversation(messages):\n",
    "    # Create a summary request\n",
    "    summary_request = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Summarize this conversation concisely.\"},\n",
    "            *messages\n",
    "        ]\n",
    "    )\n",
    "    summary = summary_request.choices[0].message.content\n",
    "    \n",
    "    # Replace the conversation with the summary\n",
    "    return [\n",
    "        messages[0],  # Keep system message\n",
    "        {\"role\": \"system\", \"content\": f\"Previous conversation summary: {summary}\"}\n",
    "    ]\n",
    "\n",
    "# When to use:\n",
    "# if len(messages) > 20:\n",
    "#     messages = summarize_conversation(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c1a42b",
   "metadata": {},
   "source": [
    "## 12. 本地LLM与OpenAI API的比较\n",
    "\n",
    "| 功能 | 本地LLM (Ollama) | OpenAI API |\n",
    "|---------|---------------------|------------|\n",
    "| 设置 | 本地下载模型 | 仅需API密钥 |\n",
    "| 成本 | 免费（下载后） | 按token付费 |\n",
    "| 隐私 | 数据保留在您的设备上 | 数据发送到OpenAI |\n",
    "| 性能 | 受您的硬件限制 | 最先进的模型 |\n",
    "| 可靠性 | 取决于您的系统 | 托管服务 |\n",
    "| 上下文窗口 | 通常较小 | 高达128K token |\n",
    "| 记忆管理 | 手动实现 | 通过API处理 |\n",
    "\n",
    "## 13. 进一步学习资源\n",
    "\n",
    "- [OpenAI API文档](https://platform.openai.com/docs/api-reference)\n",
    "- [OpenAI Cookbook](https://github.com/openai/openai-cookbook)\n",
    "- [OpenAI Python库](https://github.com/openai/openai-python)\n",
    "- [Token使用计算器](https://platform.openai.com/tokenizer)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
