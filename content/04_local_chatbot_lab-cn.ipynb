{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c507070",
   "metadata": {},
   "source": [
    "\n",
    "# 使用 Ollama 的本地大语言模型介绍\n",
    "\n",
    "是时候动手实践了！\n",
    "\n",
    "本笔记本演示如何使用 Ollama 和 LangChain 运行本地大语言模型 (LLM)。这将完全在您的计算机上运行。\n",
    "\n",
    "Ollama 允许我们在本地机器上运行 LLM。LangChain 是一个 SDK（库，即可重用代码），使与 LLM 的交互变得简单。\n",
    "\n",
    "## 1. 先决条件\n",
    "\n",
    "在运行此笔记本之前，您需要安装 Ollama 并下载一个模型。请按照以下步骤操作。\n",
    "\n",
    "### 1.1 安装 Ollama\n",
    "\n",
    "**macOS:**\n",
    "- 从 [ollama.ai](https://ollama.ai) 下载安装程序\n",
    "- 打开下载的文件并按照安装提示操作\n",
    "\n",
    "**Windows:**\n",
    "- 从 [ollama.ai](https://ollama.ai) 下载安装程序\n",
    "- 运行安装程序并按照说明操作\n",
    "\n",
    "**Linux:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed080264",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -fsSL https://ollama.ai/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197bc80b",
   "metadata": {},
   "source": [
    "验证您的安装（Windows、Mac 和 Linux）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a646af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ollama --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e419883",
   "metadata": {},
   "source": [
    "### 1.2 下载 TinyLlama 模型\n",
    "\n",
    "下载 TinyLlama 模型（约 600MB）。Ollama 有一个很大的模型目录，您可以从中下载包括 DeepSeek 和其他模型，但 TinyLlama 是最小的，非常适合演示目的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d7a962",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ollama pull tinyllama:1.1b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcd094b",
   "metadata": {},
   "source": [
    "## 2. 设置 Python 环境\n",
    "\n",
    "安装所需的包。我们将安装 Langchain，这是与 LLM 交互最流行的库之一。Langchain-ollama 允许我们与本地模型交互。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453bcfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install langchain langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bea82a",
   "metadata": {},
   "source": [
    "## 3. 使用 LangChain 与 LLM 交互\n",
    "\n",
    "下面的代码允许我们与本地计算机上托管的 tinyllama 模型交互。我们只是向模型发送以下消息：\n",
    "。\n",
    "\n",
    "如果您想确切了解发生了什么，请阅读代码注释。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d27d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "# Initialize the LLM. This is when we pick the model and the temperature (which controls the randomness of the output). llm is what we will use to call the model.\n",
    "llm = ChatOllama(\n",
    "    model=\"tinyllama:1.1b\",\n",
    "    temperature=0,  # 0 for more deterministic outputs\n",
    ")\n",
    "\n",
    "# We need to provide an array of messages to the model. The first message is always the system message, which tells the model what it is. The second message is the human message, which is what we want to ask the model.\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "\n",
    "# Send the array of messages to the model and get the response. The response is an AIMessage object, which contains the content of the message. Which we will print below.\n",
    "ai_msg = llm.invoke(messages)\n",
    "\n",
    "# Display the response\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a991df64",
   "metadata": {},
   "source": [
    "## 4. 测试不同的提示\n",
    "\n",
    "让我们尝试几个例子，通过更改消息数组并使用不同的消息来看看模型能做什么："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b09dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask for an explanation\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful and informative AI assistant.\",\n",
    "    ),\n",
    "    (\"human\", \"Explain the concept of a neural network in simple terms.\"),\n",
    "]\n",
    "\n",
    "ai_msg = llm.invoke(messages)\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f4d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask for some code\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are an expert Python programmer.\",\n",
    "    ),\n",
    "    (\"human\", \"Write a function to check if a string is a palindrome.\"),\n",
    "]\n",
    "\n",
    "ai_msg = llm.invoke(messages)\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8888f3",
   "metadata": {},
   "source": [
    "## 5. 创建基本聊天机器人\n",
    "\n",
    "让我们创建一个简单的聊天界面，这样我们就可以与模型交互。每个单元格将代表一次交互。按照以下步骤操作，当您完成这些步骤时会更有意义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce6133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Run this cell first\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOllama(\n",
    "    model=\"tinyllama:1.1b\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# System prompt\n",
    "system_prompt = \"You are a helpful assistant\"\n",
    "\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc93f76",
   "metadata": {},
   "source": [
    "现在让我们创建一个单元格来进行第一次交互，告诉模型我们最喜欢的颜色是什么。请注意，响应可能会很奇怪，我们没有使用最好的模型。但它应该表明它理解了您最喜欢的颜色是蓝色。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e34571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First interaction \n",
    "user_message = \"My favorite color is blue.\"\n",
    "\n",
    "# Create fresh messages for this interaction only\n",
    "messages = [\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", user_message)\n",
    "]\n",
    "\n",
    "# Get response\n",
    "ai_msg = llm.invoke(messages)\n",
    "\n",
    "print(f\"You: {user_message}\")\n",
    "print(f\"Assistant: {ai_msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec121dd8",
   "metadata": {},
   "source": [
    "现在让我们跟进一个问题："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7f9667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second interaction\n",
    "user_message = \"What's my favorite color?\"\n",
    "\n",
    "# Create fresh messages for this interaction only\n",
    "messages = [\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", user_message)\n",
    "]\n",
    "\n",
    "# Get response\n",
    "ai_msg = llm.invoke(messages)\n",
    "\n",
    "print(f\"You: {user_message}\")\n",
    "print(f\"Assistant: {ai_msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9a48b5",
   "metadata": {},
   "source": [
    "如您所见，模型不记得我们最喜欢的颜色！您可能会问为什么？ChatGPT 和其他工具当然可以记住对话的完整上下文。这就是记忆的概念。LLM 本身不记得过去的对话。我们需要设计我们的应用程序使其能够记住。\n",
    "\n",
    "让我们在下面探索如何做到这一点，以及像 ChatGPT 这样的工具如何管理记忆。\n",
    "\n",
    "\n",
    "## 6. 理解对话记忆的工作原理\n",
    "\n",
    "最简单的管理方法是将整个对话历史传递给模型，而不仅仅是单个消息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0c3fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "# Initialize the LLM like before\n",
    "llm = ChatOllama(\n",
    "    model=\"tinyllama:1.1b\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Define the conversation history (always starts with a system prompt)\n",
    "conversation = [SystemMessage(content=\"You are a helpful assistant\")] #conversation is an array (list) of messages. SystemMessage is a class that represents a system message and accepts content as a parameter.\n",
    "\n",
    "#Define a function called chat which takes user input as a parameter. This function will be used to interact with the model.\n",
    "def chat(user_input: str):\n",
    "    # 1) Append the user's message\n",
    "    conversation.append(HumanMessage(content=user_input)) #We add the user message to the conversation list. Now we have the system message and the user message in the conversation list.\n",
    "    \n",
    "    # 2) Call the model with the full conversation list (user message + system message)\n",
    "    ai_msg = llm(conversation)\n",
    "    \n",
    "    # 3) Display the exchange\n",
    "    print(f\"You: {user_input}\")\n",
    "    print(f\"Assistant: {ai_msg.content}\")\n",
    "    print(f\"[Conversation length: {len(conversation) + 1} messages]\")  # +1 for the AI response\n",
    "    \n",
    "    # 4) Add the AI's response to history. So now we have the system message, user message, and AI response in the conversation list which will be used for the next interaction.\n",
    "    conversation.append(ai_msg)\n",
    "    \n",
    "    return ai_msg.content\n",
    "\n",
    "print(\"Chat With Memory\")\n",
    "print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871fb1e4",
   "metadata": {},
   "source": [
    "让我们再次提供我们最喜欢的颜色："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aa893e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First interaction - share favorite color\n",
    "chat(\"My favorite color is blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f398069",
   "metadata": {},
   "source": [
    "现在让我们看看它是否记住了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621879aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second interaction - test memory\n",
    "chat(\"What's my favorite color?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f15a933",
   "metadata": {},
   "source": [
    "现在模型应该正确记住您的名字！这是因为我们在跟踪对话历史。\n",
    "\n",
    "## 7. 上下文窗口挑战\n",
    "\n",
    "虽然我们简单的记忆解决方案最初是有效的，因为我们只是将所有消息历史作为输入传递，但它有一个关键限制：**上下文窗口**。\n",
    "\n",
    "### 理解上下文窗口\n",
    "\n",
    "每个语言模型都有一个固定的\n",
    " - 它一次可以处理的最大令牌数（大致是单词或单词片段）：\n",
    "\n",
    "- **TinyLlama**: ~2,048 个令牌（约 1,500 个单词）\n",
    "- **GPT-3.5**: ~4,096 个令牌\n",
    "- **GPT-4**: ~8,192 到 128,000 个令牌，取决于版本\n",
    "- **Claude 3**: 高达 200,000 个令牌\n",
    "\n",
    "随着对话的增长，我们最终会达到这个限制。当这种情况发生时：\n",
    "\n",
    "1. 模型无法看到超出窗口大小的消息\n",
    "2. 它有效地\n",
    "对话的最早部分\n",
    "3. 较大的上下文处理变慢\n",
    "4. 如果您超出上下文窗口，可能会收到错误\n",
    "\n",
    "## 8. 上下文窗口问题的解决方案\n",
    "\n",
    "在像 ChatGPT 这样的生产系统中，有几种技术可以解决上下文窗口限制：\n",
    "\n",
    "### 1. 滑动上下文窗口\n",
    "这种方法只保留最近的 N 条消息，加上系统提示。\n",
    "\n",
    "**示例：** 如果您限制为 10 条消息，而您的对话达到 15 条消息，您会丢弃 5 条最旧的消息（除了系统提示）。\n",
    "\n",
    "这就像进行对话，您只记得最后几分钟说的话。它很简单但会丢失所有较旧的信息。\n",
    "\n",
    "### 2. 摘要\n",
    "这种技术将对话的较旧部分压缩成摘要以节省令牌空间。\n",
    "\n",
    "**示例：** 在 10 次来回交流后，系统可能会用一个摘要替换这 20 条消息：\n",
    "\n",
    "\n",
    "这保留了关键点，同时显著减少令牌使用。\n",
    "\n",
    "### 3. 基于数据库的会话管理\n",
    "\n",
    "这种方法使用数据库来存储与唯一用户会话关联的完整对话历史。工作原理如下：\n",
    "\n",
    "1. **会话创建**：当用户开始聊天时，系统创建一个唯一的会话 ID（如\n",
    "）\n",
    "\n",
    "2. **消息存储**：用户和 AI 的每条消息都存储在链接到该会话 ID 的数据库表中\n",
    "\n",
    "3. **上下文窗口管理**：在为 LLM 准备提示时，系统：\n",
    "   - 检索会话的所有消息\n",
    "   - 应用策略以适应上下文窗口（如滑动窗口或摘要）\n",
    "   - 将优化的对话发送给模型\n",
    "   - 将新响应存储回数据库\n",
    "\n",
    "**实践中的示例：**\n",
    "- 用户与 ChatGPT 聊天了几个小时\n",
    "- 他们的会话 ID \n",
    " 现在在数据库中有 200 条消息\n",
    "- 当他们发送第 201 条消息时，系统：\n",
    "  - 从数据库检索所有 200 条先前消息\n",
    "  - 选择最重要的消息以适应上下文窗口\n",
    "  - 从模型获得响应\n",
    "  - 将第 201 条消息和响应添加到数据库\n",
    "  - 即使模型只\n",
    "最近的部分，完整历史仍保留在数据库中\n",
    "\n",
    "这就是像 ChatGPT 这样的服务如何能够在非常长的对话中保持\n",
    "，甚至当您关闭浏览器并稍后返回时也是如此。\n",
    "\n",
    "## ChatGPT 通常使用什么\n",
    "\n",
    "ChatGPT 使用结合几种技术的混合方法：\n",
    "\n",
    "1. **主要技术：** 它使用非常大的上下文窗口（GPT-4o 中高达 128K 令牌）和基于数据库的会话管理，允许它\n",
    "非常长的对话。\n",
    "\n",
    "2. **对于长对话：** 当对话超出这些慷慨的限制时，它使用滑动窗口技术，优先考虑：\n",
    "   - 系统提示/指令\n",
    "   - 最近的消息\n",
    "   - 信息密度高的消息\n",
    "   - 用户明确引用的消息\n",
    "\n",
    "3. **动态压缩：** 在某些版本中，ChatGPT 还使用动态压缩算法，有选择地摘要或删除对话中似乎与当前交流不太相关的部分。\n",
    "\n",
    "虽然这些解决方案有所帮助，但任何模型在单次对话中能够\n",
    "的内容仍有绝对限制。这就是为什么即使是 ChatGPT 有时也会\n",
    "在非常长的对话中较早提到的事情。\n",
    "\n",
    "## 9. 本地 LLM 的好处\n",
    "\n",
    "使用像 Ollama 这样的工具运行本地 LLM 有几个优势：\n",
    "\n",
    "1. **隐私**: 您的数据永远不会离开您的计算机\n",
    "2. **无 API 成本**: 随心所欲地运行模型，无需按查询付费\n",
    "3. **离线使用**: 下载模型后无需互联网连接\n",
    "4. **无速率限制**: 根据您的硬件能力运行尽可能多的查询\n",
    "\n",
    "## 10. 进一步学习资源\n",
    "\n",
    "- [Ollama 文档](https://github.com/ollama/ollama/blob/main/README.md)\n",
    "- [LangChain 文档](https://python.langchain.com/docs/get_started/introduction)\n",
    "- [TinyLlama 模型信息](https://github.com/jzhang38/TinyLlama)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
