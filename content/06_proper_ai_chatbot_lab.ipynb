{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "942335d7",
   "metadata": {},
   "source": [
    "\n",
    "# Building Interactive AI Applications with Gradio\n",
    "\n",
    "This lab guides you through creating interactive AI applications using Gradio, focusing on building a customizable chatbot with streaming responses.\n",
    "\n",
    "Gradio is a python frontend library which makes it very easy to build chatbot experiences. \n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, make sure you have the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a84e9dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-5.38.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: openai in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (1.97.0)\n",
      "Requirement already satisfied: python-dotenv in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (1.1.1)\n",
      "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from gradio) (4.9.0)\n",
      "Collecting audioop-lts<1.0 (from gradio)\n",
      "  Downloading audioop_lts-0.2.1-cp313-abi3-macosx_11_0_arm64.whl.metadata (1.6 kB)\n",
      "Collecting brotli>=1.1.0 (from gradio)\n",
      "  Downloading Brotli-1.1.0-cp313-cp313-macosx_10_13_universal2.whl.metadata (5.5 kB)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
      "  Downloading fastapi-0.116.1-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.6.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gradio-client==1.11.0 (from gradio)\n",
      "  Downloading gradio_client-1.11.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio)\n",
      "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from gradio) (0.28.1)\n",
      "Collecting huggingface-hub>=0.28.1 (from gradio)\n",
      "  Downloading huggingface_hub-0.33.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting jinja2<4.0 (from gradio)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting markupsafe<4.0,>=2.0 (from gradio)\n",
      "  Downloading MarkupSafe-3.0.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting numpy<3.0,>=1.0 (from gradio)\n",
      "  Downloading numpy-2.3.1-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: orjson~=3.0 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from gradio) (3.11.0)\n",
      "Requirement already satisfied: packaging in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from gradio) (25.0)\n",
      "Collecting pandas<3.0,>=1.0 (from gradio)\n",
      "  Downloading pandas-2.3.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting pillow<12.0,>=8.0 (from gradio)\n",
      "  Downloading pillow-11.3.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from gradio) (2.11.7)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from gradio) (6.0.2)\n",
      "Collecting ruff>=0.9.3 (from gradio)\n",
      "  Downloading ruff-0.12.4-py3-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
      "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.47.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio)\n",
      "  Downloading typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from gradio) (4.14.1)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting fsspec (from gradio-client==1.11.0->gradio)\n",
      "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting websockets<16.0,>=10.0 (from gradio-client==1.11.0->gradio)\n",
      "  Downloading websockets-15.0.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: certifi in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas<3.0,>=1.0->gradio)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas<3.0,>=1.0->gradio)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
      "Collecting click>=8.0.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from openai) (4.67.1)\n",
      "Collecting filelock (from huggingface-hub>=0.28.1->gradio)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: requests in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from huggingface-hub>=0.28.1->gradio) (2.32.4)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub>=0.28.1->gradio)\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-macosx_11_0_arm64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: six>=1.5 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.5.0)\n",
      "Downloading gradio-5.38.0-py3-none-any.whl (59.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.11.0-py3-none-any.whl (324 kB)\n",
      "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading audioop_lts-0.2.1-cp313-abi3-macosx_11_0_arm64.whl (26 kB)\n",
      "Downloading fastapi-0.116.1-py3-none-any.whl (95 kB)\n",
      "Downloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp313-cp313-macosx_11_0_arm64.whl (12 kB)\n",
      "Downloading numpy-2.3.1-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.1-cp313-cp313-macosx_11_0_arm64.whl (10.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.3.0-cp313-cp313-macosx_11_0_arm64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading starlette-0.47.1-py3-none-any.whl (72 kB)\n",
      "Downloading tomlkit-0.13.3-py3-none-any.whl (38 kB)\n",
      "Downloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Downloading websockets-15.0.1-cp313-cp313-macosx_11_0_arm64.whl (173 kB)\n",
      "Downloading Brotli-1.1.0-cp313-cp313-macosx_10_13_universal2.whl (815 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.7/815.7 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n",
      "Downloading hf_xet-1.1.5-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading ruff-0.12.4-py3-none-macosx_11_0_arm64.whl (10.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Downloading ffmpy-0.6.0-py3-none-any.whl (5.5 kB)\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pytz, pydub, brotli, websockets, tzdata, tomlkit, shellingham, semantic-version, ruff, python-multipart, pillow, numpy, mdurl, markupsafe, hf-xet, groovy, fsspec, filelock, ffmpy, click, audioop-lts, aiofiles, uvicorn, starlette, pandas, markdown-it-py, jinja2, huggingface-hub, safehttpx, rich, gradio-client, fastapi, typer, gradio\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34/34\u001b[0m [gradio]33/34\u001b[0m [gradio]]ace-hub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiofiles-24.1.0 audioop-lts-0.2.1 brotli-1.1.0 click-8.2.1 fastapi-0.116.1 ffmpy-0.6.0 filelock-3.18.0 fsspec-2025.7.0 gradio-5.38.0 gradio-client-1.11.0 groovy-0.1.2 hf-xet-1.1.5 huggingface-hub-0.33.4 jinja2-3.1.6 markdown-it-py-3.0.0 markupsafe-3.0.2 mdurl-0.1.2 numpy-2.3.1 pandas-2.3.1 pillow-11.3.0 pydub-0.25.1 python-multipart-0.0.20 pytz-2025.2 rich-14.0.0 ruff-0.12.4 safehttpx-0.1.6 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.47.1 tomlkit-0.13.3 typer-0.16.0 tzdata-2025.2 uvicorn-0.35.0 websockets-15.0.1\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install gradio openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580ae89a",
   "metadata": {},
   "source": [
    "## 1. Setting Up Your Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aaabe25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (if using .env file)\n",
    "load_dotenv()\n",
    "\n",
    "# IMPORTANT: Set your API key directly (for testing only)\n",
    "# NOTE: For security reasons, you should use a .env file instead in production\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "# Now initialize the OpenAI client\n",
    "\n",
    "# Verify API key\n",
    "if os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"OpenAI API Key loaded successfully\")\n",
    "else:\n",
    "    print(\"ERROR: OpenAI API Key not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d63286",
   "metadata": {},
   "source": [
    "## 2. Your First Gradio Interface\n",
    "\n",
    "Let's create a simple Gradio interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cb51446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will define a simple function called echo, all it does is it will accept a message \n",
    "# and simply return it back!\n",
    "def echo(message):\n",
    "    return f\"You said: {message}\"\n",
    "\n",
    "# Create a basic Gradio interface. gr.interface() is a function that creates a web-based UI \n",
    "# for your function, basically what that means is that it will create a web page where you \n",
    "# can enter text and see the output. That interface will be hosted on a local server, and \n",
    "# you can access it via your web browser.\n",
    "# The interface accepts the function to be called (echo in this case), the input type \n",
    "# (textbox), and the output type (also textbox). The title and description are optional, \n",
    "# but they help to provide context for the user. Flagging mode is set to \"never\" to \n",
    "# disable the flagging feature to make it look better.\n",
    "# When you run this function, it will spin up an interface with input and output boxes, \n",
    "# and a button to submit the input. When you submit the input, it will call the echo \n",
    "# function with the input text and display the output in the output box.\n",
    "demo = gr.Interface(\n",
    "    fn=echo,\n",
    "    inputs=\"textbox\",\n",
    "    outputs=\"textbox\",\n",
    "    title=\"Echo Bot\",\n",
    "    description=\"A simple bot that echoes what you type\",\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a73821d",
   "metadata": {},
   "source": [
    "## 3. Building a Basic Chatbot\n",
    "\n",
    "Let's create a simple chatbot using OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8427d3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to get response from OpenAI. We already went through that before. \n",
    "def get_response(message):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# We will create the same exact gradio interface as before, but we will use get_response \n",
    "# function instead of echo! The get_response function will call the Open AI with the \n",
    "# message and return the response. The response will be displayed in the output box.\n",
    "# Basically, the user will enter a message in the input box, and when they click the \n",
    "# submit button, it will call the get_response function with the input text and display \n",
    "# the output in the output box.\n",
    "\n",
    "chatbot = gr.Interface(\n",
    "    fn=get_response,\n",
    "    inputs=gr.Textbox(placeholder=\"Ask me anything...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Simple AI Chatbot\",\n",
    "    description=\"Chat with an AI assistant powered by OpenAI\",\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "\n",
    "# Launch the chatbot\n",
    "chatbot.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68d20ad",
   "metadata": {},
   "source": [
    "## 4. Creating a Streaming Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e408435c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to stream responses from OpenAI\n",
    "def stream_response(message):\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            response += chunk.choices[0].delta.content\n",
    "            # yield is a python keyword that allows you to return a value from a function \n",
    "            # without terminating the function. This means that the function can be called \n",
    "            # again later, and it will continue from where it left off. This is useful \n",
    "            # for streaming responses, as it allows you to return partial results as they \n",
    "            # are generated. So this is instead of return, which would terminate the function.\n",
    "            yield response \n",
    "\n",
    "# Create a streaming chatbot interface\n",
    "streaming_chatbot = gr.Interface(\n",
    "    fn=stream_response,\n",
    "    inputs=\"textbox\",\n",
    "    outputs=\"text\",\n",
    "    title=\"Streaming AI Chatbot\",\n",
    "    description=\"Chat with an AI assistant that streams responses in real-time\",\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "\n",
    "# Launch the streaming chatbot\n",
    "streaming_chatbot.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61e1d00",
   "metadata": {},
   "source": [
    "## 5. Building a Chatbot with Memory\n",
    "\n",
    "Now let's create a more advanced chatbot that remembers conversation history. \n",
    "\n",
    "Again, we already went through that process before, we are just adding it to a Gradio interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a9ae82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages/gradio/chat_interface.py:345: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function called chat, which accepts the user message and conversation history\n",
    "def chat(message, history):\n",
    "    # Initialize messages with system prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "    \n",
    "    # We are looping through the history of messages (initially just an empty list). \n",
    "    # Entry is a single message in the history\n",
    "    for entry in history:\n",
    "        # Check if entry is in list format [user_msg, assistant_msg]\n",
    "        if isinstance(entry, list) and len(entry) == 2:\n",
    "            user_msg, assistant_msg = entry\n",
    "            # Add user message\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            # Add assistant message if it exists\n",
    "            if assistant_msg:\n",
    "                messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "        # If history is already in OpenAI format with role and content\n",
    "        elif isinstance(entry, dict) and \"role\" in entry and \"content\" in entry:\n",
    "            messages.append(entry)\n",
    "    \n",
    "    # Add the new user message to the conversation\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "    \n",
    "    # Get streaming response from OpenAI\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    # Return the response chunk by chunk as it arrives\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            response += chunk.choices[0].delta.content\n",
    "            yield response\n",
    "\n",
    "# Create and launch the chatbot interface\n",
    "memory_chatbot = gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    title=\"AI Chatbot with Memory\",\n",
    "    description=\"Chat with an AI assistant that remembers your conversation history.\",\n",
    "    examples=[\"Tell me about machine learning\", \"How do neural networks work?\"],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "\n",
    "memory_chatbot.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3a8177",
   "metadata": {},
   "source": [
    "## 6. Restaurant Menu Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a4b457d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to generate a restaurant menu\n",
    "def generate_menu(restaurant_name, cuisine_type, special_requirements=\"None\"):\n",
    "    prompt = f\"\"\"\n",
    "    Create a restaurant menu for \"{restaurant_name}\", a {cuisine_type} restaurant.\n",
    "    Special requirements: {special_requirements}\n",
    "    \n",
    "    Include:\n",
    "    - 3 appetizers\n",
    "    - 4 main courses\n",
    "    - 2 desserts\n",
    "    \n",
    "    For each item, include a name, brief description, and price.\n",
    "    \n",
    "    Format your response in markdown with appropriate headers, styling, and sections.\n",
    "    Add a brief introduction about the restaurant at the top.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert restaurant consultant who creates beautiful, well-formatted menus.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Create a menu generator interface\n",
    "menu_generator = gr.Interface(\n",
    "    fn=generate_menu,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Restaurant Name\"),\n",
    "        gr.Textbox(label=\"Cuisine Type (e.g., Italian, Japanese)\"),\n",
    "        gr.Textbox(label=\"Special Requirements (Optional)\", placeholder=\"e.g., vegetarian options\")\n",
    "    ],\n",
    "    outputs=gr.Markdown(label=\"Generated Menu\"),\n",
    "    title=\"Restaurant Menu Generator\",\n",
    "    description=\"Create a professional restaurant menu with AI\",\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "\n",
    "# Launch the menu generator\n",
    "menu_generator.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7036418f",
   "metadata": {},
   "source": [
    "## 7. Resources for Further Learning\n",
    "\n",
    "- [Gradio Documentation](https://www.gradio.app/docs/)\n",
    "- [OpenAI API Documentation](https://platform.openai.com/docs/api-reference)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
