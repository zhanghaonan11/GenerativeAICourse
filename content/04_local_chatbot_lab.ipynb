{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6feea88",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "\n",
    "# Introduction to Local LLMs with Ollama\n",
    "\n",
    "Time to get practical!\n",
    "\n",
    "This notebook demonstrates how to run a local Large Language Model (LLM) using Ollama and LangChain. This will run entirely on your computer. \n",
    "\n",
    "Ollama allows us to run LLMs locally on our machine. LangChain is an SDK (library i.e. re-usable code) which makes it easy to interact with LLMs. \n",
    "\n",
    "## 1. Prerequisites\n",
    "\n",
    "Before running this notebook, you need to install Ollama and download a model. Follow the steps below.\n",
    "\n",
    "### 1.1 Installing Ollama\n",
    "\n",
    "**macOS:**\n",
    "- Download the installer from [ollama.ai](https://ollama.ai)\n",
    "- Open the downloaded file and follow the installation prompts\n",
    "\n",
    "**Windows:**\n",
    "- Download the installer from [ollama.ai](https://ollama.ai)\n",
    "- Run the installer and follow the instructions\n",
    "\n",
    "**Linux:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ed4976",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -fsSL https://ollama.ai/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9a2cd7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Verify your installation (Windows, Mac, and Linux):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6da91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ollama --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeefb3e1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### 1.2 Downloading the TinyLlama Model\n",
    "\n",
    "Download the TinyLlama model (about 600MB). There is a big catalog of models you can download from Ollama including DeepSeek and others, but TinyLlama is the smallest, which is good for demo purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0178721",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ollama pull tinyllama:1.1b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd55eab0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## 2. Setting Up the Python Environment\n",
    "\n",
    "Install the required packages. We will install Langchain, which is one of the most popular libraries to interacat with LLMs. Langchain-ollama allows us to interact with models locally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c100aa65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (0.3.26)\n",
      "Requirement already satisfied: langchain-ollama in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (0.3.5)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from langchain) (0.3.69)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from langchain) (0.4.7)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain) (2025.7.14)\n",
      "Requirement already satisfied: ollama<1.0.0,>=0.5.1 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from langchain-ollama) (0.5.1)\n",
      "Requirement already satisfied: httpx>=0.27 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from ollama<1.0.0,>=0.5.1->langchain-ollama) (0.28.1)\n",
      "Requirement already satisfied: anyio in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.1->langchain-ollama) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.1->langchain-ollama) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.27->ollama<1.0.0,>=0.5.1->langchain-ollama) (0.16.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (3.11.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from anyio->httpx>=0.27->ollama<1.0.0,>=0.5.1->langchain-ollama) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install langchain langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5feaaf",
   "metadata": {},
   "source": [
    "## 3. Using the LLM with LangChain\n",
    "\n",
    "The code below allows us to interact with the tinyllama model hosted locally on our computers. We are simply sending the following message to the model: \"I love programming\".\n",
    "\n",
    "Please read the code comments if you want to understand exactly what's going on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "989512b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm glad to hear that you enjoy programming! Here are some tips and resources to help you get started:\n",
      "\n",
      "1. Learn the basics of programming: there are many online tutorials, books, and courses available that can help you learn the fundamentals of programming. Start with simple projects like creating a basic program in a language like python or javascript. 2. Join a community: join a programming community on social media or through a website like codecademy. This will give you access to a vast pool of resources and people who can help you learn and grow as a programmer. 3. Practice, practice, practice: the more you practice, the better you'll get at coding. Try to write your own programs and see how they work. You can also join online coding challenges or competitions to test your skills and improve your knowledge. 4. Take courses: if you have the time and money, consider taking a course in a programming language that interests you. This will give you a solid foundation of knowledge and help you learn new techniques and concepts. 5. Read books: there are many great books on programming available online or at bookstores. Some popular ones include \"the cathedral and the bazaar\" by steve jobs, \"programming ruby\" by rachel grimes, and \"c# from the ground up\" by james gillis. Remember that learning to code is a journey, so don't be too hard on yourself if you make mistakes or don't understand something at first. Keep practicing and learning, and you'll get better with time!\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "# Initialize the LLM. This is when we pick the model and the temperature (which controls the randomness of the output). llm is what we will use to call the model.\n",
    "llm = ChatOllama(\n",
    "    model=\"tinyllama:1.1b\",\n",
    "    temperature=0,  # 0 for more deterministic outputs\n",
    ")\n",
    "\n",
    "# We need to provide an array of messages to the model. The first message is always the system message, which tells the model what it is. The second message is the human message, which is what we want to ask the model.\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "\n",
    "# Send the array of messages to the model and get the response. The response is an AIMessage object, which contains the content of the message. Which we will print below.\n",
    "ai_msg = llm.invoke(messages)\n",
    "\n",
    "# Display the response\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71ab5a6",
   "metadata": {},
   "source": [
    "## 4. Testing Different Prompts\n",
    "\n",
    "Let's try a couple of examples to see what the model can do by changing the messaging array and using a different message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0aa4acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A neural network is a type of artificial intelligence (AI) system that mimics the way human brains process information. It consists of several layers of interconnected neurons, also known as nodes or units, which receive input data and produce output based on their connections to other nodes. The input data can be either numerical or categorical, and it is passed through a series of layers until it reaches the output layer.\n",
      "\n",
      "In a neural network, each node in the network has a set of inputs (or weights) that determine its output. These weights are learned from training data, which consists of examples where each input is paired with an output. The weights are updated based on the output received by the neuron, and this process is repeated for all the nodes in the network.\n",
      "\n",
      "The output of a neural network can be used to make predictions or classifications, depending on the type of task being performed. For example, a neural network trained on image classification tasks might produce an output that indicates whether an image belongs to a particular category (e.g., cat vs. Dog).\n",
      "\n",
      "Neural networks are often used in machine learning and deep learning applications, where they can be trained using supervised or unsupervised learning techniques. They have been applied in fields such as healthcare, finance, and marketing, among others.\n"
     ]
    }
   ],
   "source": [
    "# Ask for an explanation\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful and informative AI assistant.\",\n",
    "    ),\n",
    "    (\"human\", \"Explain the concept of a neural network in simple terms.\"),\n",
    "]\n",
    "\n",
    "ai_msg = llm.invoke(messages)\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaf603f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a Python function that checks whether a given string is a palindrome or not:\n",
      "\n",
      "```python\n",
      "def is_palindrome(str):\n",
      "    \"\"\"\n",
      "    Checks whether a given string is a palindrome (reversed) or not.\n",
      "    \n",
      "    Args:\n",
      "        str (string): The string to check for palindromicity.\n",
      "        \n",
      "    Returns:\n",
      "        bool: True if the string is a palindrome, False otherwise.\n",
      "    \"\"\"\n",
      "    # Convert the string to lowercase and remove any leading or trailing spaces\n",
      "    str = str.lower().strip()\n",
      "    \n",
      "    # Check if the string is empty or contains only whitespace characters\n",
      "    if not str:\n",
      "        return True\n",
      "    \n",
      "    # Reverse the string and check if it's equal to the original string\n",
      "    reversed_str = str[::-1]\n",
      "    if reversed_str == str:\n",
      "        return True\n",
      "    else:\n",
      "        return False\n",
      "```\n",
      "\n",
      "Here's an example usage:\n",
      "\n",
      "```python\n",
      ">>> is_palindrome(\"race a car\")\n",
      "True\n",
      ">>> is_palindrome(\"race a car\")\n",
      "False\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Ask for some code\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are an expert Python programmer.\",\n",
    "    ),\n",
    "    (\"human\", \"Write a function to check if a string is a palindrome.\"),\n",
    "]\n",
    "\n",
    "ai_msg = llm.invoke(messages)\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed492d41",
   "metadata": {},
   "source": [
    "## 5. Creating a Basic Chatbot\n",
    "\n",
    "Let's create a simple chat interface so we can interact with the model. Each cell will represent an interaction. Follow the steps below, they will make more sense when you go through them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3238d483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Setup: Run this cell first\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOllama(\n",
    "    model=\"tinyllama:1.1b\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# System prompt\n",
    "system_prompt = \"You are a helpful assistant\"\n",
    "\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1c0b56",
   "metadata": {},
   "source": [
    "Now let's create a cell for our first interaction by telling the model what our favorite color is. Note that the response might be weird, again we're not using the best model out there. But it should indicate that it understood that your favorite color is blue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db32f77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: My favorite color is blue.\n",
      "Assistant: I do not have the ability to feel emotions or preferences like humans do. However, based on your statement that you enjoy wearing blue clothing, I can say that blue is a popular and versatile color that can be worn in various ways. It's often associated with nature, peacefulness, and calmness, making it a great choice for those who prefer a calming and relaxing atmosphere. In terms of fashion, blue can be paired with different colors to create a unique and eye-catching look. Whether you choose to wear it as a base color or add some accents like stripes or prints, blue is a versatile and stylish choice that can complement any outfit.\n"
     ]
    }
   ],
   "source": [
    "# First interaction \n",
    "user_message = \"My favorite color is blue.\"\n",
    "\n",
    "# Create fresh messages for this interaction only\n",
    "messages = [\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", user_message)\n",
    "]\n",
    "\n",
    "# Get response\n",
    "ai_msg = llm.invoke(messages)\n",
    "\n",
    "print(f\"You: {user_message}\")\n",
    "print(f\"Assistant: {ai_msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc9f698",
   "metadata": {},
   "source": [
    "Now let's follow up with a question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7c242b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: What's my favorite color?\n",
      "Assistant: I do not have the ability to have feelings or preferences like humans. However, based on your previous responses and comments, you may be interested in learning about some popular colors that are commonly associated with different emotions and personality traits. Some examples include:\n",
      "\n",
      "1. Blue - calming, peaceful, trustworthy, introspective, reflective, spiritual, and calm\n",
      "2. Green - growth, renewal, nature, freshness, and new beginnings\n",
      "3. Yellow - optimism, sunshine, happiness, cheerfulness, playfulness, and warmth\n",
      "4. Red - passion, energy, excitement, fiery, passionate, and fierce\n",
      "5. Purple - royalty, mystical, mysterious, enigmatic, mysterious, and regal\n",
      "6. Orange - enthusiasm, energy, vitality, happiness, cheerfulness, and warmth\n",
      "7. Pink - femininity, sweetness, romance, softness, tender, and delicate\n",
      "8. Gold - luxury, wealth, prosperity, success, shiny, and glamorous\n",
      "9. Brown - earthy, grounded, stable, reliable, dependable, and solid\n",
      "10. Black - mystery, secrecy, darkness, moodiness, mysterious, and mysterious\n",
      "\n",
      "these are just a few examples, but there are many more colors that can evoke different emotions depending on the context and personality of the individual.\n"
     ]
    }
   ],
   "source": [
    "# Second interaction\n",
    "user_message = \"What's my favorite color?\"\n",
    "\n",
    "# Create fresh messages for this interaction only\n",
    "messages = [\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", user_message)\n",
    "]\n",
    "\n",
    "# Get response\n",
    "ai_msg = llm.invoke(messages)\n",
    "\n",
    "print(f\"You: {user_message}\")\n",
    "print(f\"Assistant: {ai_msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e27722d",
   "metadata": {},
   "source": [
    "As you can see, the model doesn't remember our favorite color! You might ask why? Surely tools like ChatGPT and others can remember the full context of the conversation. This is the concept of memory. The LLM, by itself, does not remember past conversations. We need to engineer our application so that it does.\n",
    "\n",
    "Let us explore how we can do it below, and how tools like ChatGPT manage that. \n",
    "\n",
    "\n",
    "## 6. Understanding How Conversation Memory Works\n",
    "\n",
    "The easiest way to manage that is by simply passing the entire conversation history to the model, instead of just the individual message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "365adc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat With Memory\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "# Initialize the LLM like before\n",
    "llm = ChatOllama(\n",
    "    model=\"tinyllama:1.1b\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Define the conversation history (always starts with a system prompt)\n",
    "conversation = [SystemMessage(content=\"You are a helpful assistant\")] #conversation is an array (list) of messages. SystemMessage is a class that represents a system message and accepts content as a parameter.\n",
    "\n",
    "#Define a function called chat which takes user input as a parameter. This function will be used to interact with the model.\n",
    "def chat(user_input: str):\n",
    "    # 1) Append the user's message\n",
    "    conversation.append(HumanMessage(content=user_input)) #We add the user message to the conversation list. Now we have the system message and the user message in the conversation list.\n",
    "    \n",
    "    # 2) Call the model with the full conversation list (user message + system message)\n",
    "    ai_msg = llm(conversation)\n",
    "    \n",
    "    # 3) Display the exchange\n",
    "    print(f\"You: {user_input}\")\n",
    "    print(f\"Assistant: {ai_msg.content}\")\n",
    "    print(f\"[Conversation length: {len(conversation) + 1} messages]\")  # +1 for the AI response\n",
    "    \n",
    "    # 4) Add the AI's response to history. So now we have the system message, user message, and AI response in the conversation list which will be used for the next interaction.\n",
    "    conversation.append(ai_msg)\n",
    "    \n",
    "    return ai_msg.content\n",
    "\n",
    "print(\"Chat With Memory\")\n",
    "print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e3433e",
   "metadata": {},
   "source": [
    "Let's give our favorite color again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a81d27fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/86/clj4kb5d4n54kw6wcnwz6rnr0000gn/T/ipykernel_44952/2493566242.py:19: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  ai_msg = llm(conversation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: My favorite color is blue\n",
      "Assistant: I'm not capable of having feelings or preferences like humans do. However, based on the given text, it seems that the speaker enjoys wearing blue clothing or attire. In general, blue is a popular and versatile color that can be used in various settings and styles. It can be worn for casual outfits, formal events, or even as a fashion statement. Blue is also a symbol of calmness, trustworthiness, and wisdom, making it an excellent choice for individuals who seek to express their unique personalities through their clothing choices.\n",
      "[Conversation length: 3 messages]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm not capable of having feelings or preferences like humans do. However, based on the given text, it seems that the speaker enjoys wearing blue clothing or attire. In general, blue is a popular and versatile color that can be used in various settings and styles. It can be worn for casual outfits, formal events, or even as a fashion statement. Blue is also a symbol of calmness, trustworthiness, and wisdom, making it an excellent choice for individuals who seek to express their unique personalities through their clothing choices.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First interaction - share favorite color\n",
    "chat(\"My favorite color is blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ac723",
   "metadata": {},
   "source": [
    "Now let's see if it remembers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "526f140f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: What's my favorite color?\n",
      "Assistant: I do not have the ability to have feelings or preferences like humans do. However, based on the given text, it seems that the speaker enjoys wearing blue clothings or attire. In general, blue is a popular and versatile color that can be used in various settings and styles. It can be worn for casual outfit, formal events, or even as a fashion statement. Blue is also associated with calmness, trustworthiness, and wisdom, making it an excellent choice for individuals who seek to express their unique personalities through their clothings choices.\n",
      "[Conversation length: 5 messages]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I do not have the ability to have feelings or preferences like humans do. However, based on the given text, it seems that the speaker enjoys wearing blue clothings or attire. In general, blue is a popular and versatile color that can be used in various settings and styles. It can be worn for casual outfit, formal events, or even as a fashion statement. Blue is also associated with calmness, trustworthiness, and wisdom, making it an excellent choice for individuals who seek to express their unique personalities through their clothings choices.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second interaction - test memory\n",
    "chat(\"What's my favorite color?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2cc0ce",
   "metadata": {},
   "source": [
    "Now the model should correctly remember your name! This is because we're keeping track of the conversation history.\n",
    "\n",
    "## 7. The Context Window Challenge\n",
    "\n",
    "While our simple memory solution works initially, because we simply passed all message history as an input, it has a critical limitation: **the context window**.\n",
    "\n",
    "### Understanding the Context Window\n",
    "\n",
    "Every language model has a fixed \"context window\" - the maximum number of tokens (roughly words or word pieces) it can process at once:\n",
    "\n",
    "- **TinyLlama**: ~2,048 tokens (about 1,500 words)\n",
    "- **GPT-3.5**: ~4,096 tokens\n",
    "- **GPT-4**: ~8,192 to 128,000 tokens depending on the version\n",
    "- **Claude 3**: Up to 200,000 tokens\n",
    "\n",
    "As the conversation grows, we eventually hit this limit. When that happens:\n",
    "\n",
    "1. The model can't see messages beyond the window size\n",
    "2. It effectively \"forgets\" the earliest parts of the conversation \n",
    "3. Processing becomes slower with larger contexts\n",
    "4. You may receive errors if you exceed the context window\n",
    "\n",
    "## 8. Solutions to the Context Window Problem\n",
    "\n",
    "In production systems like ChatGPT, several techniques address the context window limitation:\n",
    "\n",
    "### 1. Sliding Context Window\n",
    "This approach keeps only the most recent N messages, plus the system prompt.\n",
    "\n",
    "**Example:** If you limit to 10 messages, and your conversation reaches 15 messages, you'd drop the 5 oldest messages (except the system prompt).\n",
    "\n",
    "This is like having a conversation where you remember only what was said in the last few minutes. It's simple but loses all older information.\n",
    "\n",
    "### 2. Summarization\n",
    "This technique condenses older parts of the conversation into summaries to save token space.\n",
    "\n",
    "**Example:** After 10 back-and-forth exchanges, the system might replace those 20 messages with a single summary: \"User introduced themselves as Alex and asked about neural networks. Assistant explained neural networks and provided a Python code example for palindrome detection.\"\n",
    "\n",
    "This preserves the key points while reducing token usage significantly.\n",
    "\n",
    "### 3. Database-Backed Session Management\n",
    "\n",
    "This approach uses a database to store complete conversation histories associated with unique user sessions. Here's how it works:\n",
    "\n",
    "1. **Session Creation**: When a user starts chatting, the system creates a unique session ID (like \"session_abc123\")\n",
    "\n",
    "2. **Message Storage**: Every message from the user and AI is stored in a database table linked to that session ID\n",
    "\n",
    "3. **Context Window Management**: When preparing the prompt for the LLM, the system:\n",
    "   - Retrieves all messages for the session\n",
    "   - Applies strategies to fit within the context window (like sliding window or summarization)\n",
    "   - Sends the optimized conversation to the model\n",
    "   - Stores the new response back in the database\n",
    "\n",
    "**Example in Practice:**\n",
    "- A user has chatted for hours with ChatGPT\n",
    "- Their session ID \"user_789\" now has 200 messages in the database\n",
    "- When they send message #201, the system:\n",
    "  - Retrieves all 200 previous messages from the database\n",
    "  - Selects the most important ones to fit in the context window\n",
    "  - Gets a response from the model\n",
    "  - Adds message #201 and the response to the database\n",
    "  - Even if the model only \"sees\" the recent portion, the full history remains in the database\n",
    "\n",
    "This is how services like ChatGPT can maintain \"memory\" across very long conversations and even when you close your browser and come back later.\n",
    "\n",
    "## What ChatGPT Typically Uses\n",
    "\n",
    "ChatGPT uses a hybrid approach that combines several of these techniques:\n",
    "\n",
    "1. **Primary Technique:** It uses a very large context window (up to 128K tokens in GPT-4o) and database-backed session management, allowing it to \"remember\" remarkably long conversations.\n",
    "\n",
    "2. **For Long Conversations:** When conversations exceed even these generous limits, it employs sliding window techniques that prioritize:\n",
    "   - The system prompt/instructions\n",
    "   - The most recent messages\n",
    "   - Messages with high information density\n",
    "   - Messages explicitly referenced by the user\n",
    "\n",
    "3. **Dynamic Compression:** In some versions, ChatGPT also employs dynamic compression algorithms that selectively summarize or remove parts of the conversation that appear less relevant to the current exchange.\n",
    "\n",
    "While these solutions help, there's still an absolute limit to what any model can \"remember\" in a single conversation. This is why even ChatGPT sometimes \"forgets\" things mentioned much earlier in very long conversations.\n",
    "\n",
    "## 9. Benefits of Local LLMs\n",
    "\n",
    "Using local LLMs with tools like Ollama has several advantages:\n",
    "\n",
    "1. **Privacy**: Your data never leaves your computer\n",
    "2. **No API costs**: Run the model as much as you want without paying per query\n",
    "3. **Offline usage**: No internet connection required once the model is downloaded\n",
    "4. **No rate limits**: Run as many queries as your hardware can handle\n",
    "\n",
    "## 10. Resources for Further Learning\n",
    "\n",
    "- [Ollama Documentation](https://github.com/ollama/ollama/blob/main/README.md)\n",
    "- [LangChain Documentation](https://python.langchain.com/docs/get_started/introduction)\n",
    "- [TinyLlama Model Information](https://github.com/jzhang38/TinyLlama)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
