{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb2a0fd0",
   "metadata": {},
   "source": [
    "\n",
    "# Using OpenAI API with Python\n",
    "\n",
    "We went through the process of running LLMs locally on our machines.\n",
    "\n",
    "This notebook demonstrates how to interact with OpenAI's powerful language models through their official API using the chat completions endpoint.\n",
    "\n",
    "## 1. Prerequisites\n",
    "\n",
    "Before running this notebook, you need to set up your OpenAI API access.\n",
    "\n",
    "### 1.1 Getting an OpenAI API Key\n",
    "\n",
    "To use the OpenAI API, you need an API key:\n",
    "\n",
    "1. Create an account at [OpenAI's website](https://openai.com/)\n",
    "2. Navigate to the [API keys page](https://platform.openai.com/api-keys)\n",
    "3. Create a new secret key\n",
    "4. Store this key securely - it's like a password!\n",
    "\n",
    "### 1.2 Setting Up Your Environment\n",
    "\n",
    "First, install the OpenAI Python package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be6a1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0763dd",
   "metadata": {},
   "source": [
    "Then, set up your API key. For security, it's best to use environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63831fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Now os.getenv() can find the key from your .env file\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc743e92",
   "metadata": {},
   "source": [
    "## 2. Using the Chat Completions API\n",
    "\n",
    "The Chat Completions API is provided by Open AI and is designed for conversational interactions. As an input to the API, we provide the system message (how model should behave) and user message (input from user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11788be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the chat completion create method which accepts a list of messages and the model name. It returns a response object.\n",
    "# The list of messages is always an array (list) of dictionaries with two keys: role and content. The first message is always the system message, which sets the behavior of the assistant. The second message is the user message, which is the input from the user.\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a one-sentence bedtime story about a unicorn.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content) #We print the content of the first choice in the response object. The response object contains a list of choices, and each choice has a message with the content we want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dfed48",
   "metadata": {},
   "source": [
    "## 3. Understanding the Response Structure\n",
    "\n",
    "The API returns a structured response with useful metadata which we will print:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafe541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print the full response structure\n",
    "print(\"Full response object:\")\n",
    "print(response)\n",
    "\n",
    "# Access specific parts\n",
    "print(\"\\nJust the message content:\")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "print(\"\\nModel used:\")\n",
    "print(response.model)\n",
    "\n",
    "print(\"\\nUsage statistics:\")\n",
    "print(f\"Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Completion tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cbe241",
   "metadata": {},
   "source": [
    "As you can see we have useful data from the response, more importantly, we have an understanding on how much tokens this call leveraged. Models are typically priced by a combination of the input and output tokens, so in this case, we have paid for 54 tokens. We will discuss pricing in more details at the end of this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99f65d0",
   "metadata": {},
   "source": [
    "## 4. Creating a Simple Chat Interface (Without Memory)\n",
    "\n",
    "Let's build a chat interface without memory.\n",
    "\n",
    "In a notebook environment, we can't interact in a kind of chat interface where we provide input, get output, and then provide input again i.e. using an input() loop.\n",
    "\n",
    "Instead, let's create a simple function and use it in separate cells to simulate a chat without memory:\n",
    "\n",
    "### Cell 1: Define the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e6dbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function accepts a user message as an input (we will type that in the next cell) and returns a response from the OpenAI API.\n",
    "def get_response_no_memory(user_message):\n",
    "    \"\"\"Get a response from OpenAI (no conversation history)\"\"\" \n",
    "    #\"\"\" This is a docstring. It describes the function and its parameters.\"\"\" Very useful for documentation and understanding the code.\n",
    "    # Sometimes docstrings are not necessary, but they are very useful for understanding the code. It is like commenting the code, but in a more structured way.\n",
    "\n",
    "    #We call the model with the user message which we will set below\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90c6ea6",
   "metadata": {},
   "source": [
    "### Cell 2: First interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3391e7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First question (you can insert anything)\n",
    "question = \"What is artificial intelligence?\"\n",
    "answer = get_response_no_memory(question)\n",
    "\n",
    "print(f\"You: {question}\")\n",
    "print(f\"Assistant: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d9856a",
   "metadata": {},
   "source": [
    "### Cell 3: Follow-up question (without memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba3c502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second question (the AI won't remember the previous interaction)\n",
    "question = \"Can you elaborate more on that?\"\n",
    "answer = get_response_no_memory(question)\n",
    "\n",
    "print(f\"You: {question}\")\n",
    "print(f\"Assistant: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129fad52",
   "metadata": {},
   "source": [
    "You'll notice that when you ask \"Can you elaborate more on that?\" the assistant doesn't know what \"that\" refers to, because it has no memory of the previous exchange.\n",
    "\n",
    "## 5. Creating a Chat Interface With Memory\n",
    "\n",
    "Now let's build a version that maintains conversation history:\n",
    "\n",
    "### Cell 1: Setup conversation memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e03bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize conversation with system message. We will add more messages to the conversation memory (history) as we go.\n",
    "conversation_memory = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "#Function to add user message to conversation history and get a response from OpenAI\n",
    "def chat_with_memory(user_message):\n",
    "    \"\"\"Chat with the AI while maintaining conversation history\"\"\"\n",
    "    \n",
    "    # Add user message to history. Now, the conversation memory contains the system message and the user message.\n",
    "    conversation_memory.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    # Get response from OpenAI\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=conversation_memory\n",
    "    )\n",
    "    \n",
    "    # Extract assistant's response\n",
    "    assistant_response = response.choices[0].message.content\n",
    "    \n",
    "    # Add assistant response to conversation history. This is the response from the AI. This will be added to the conversation memory and will be usedin the future.\n",
    "    conversation_memory.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "    \n",
    "    # Return the response and token usage\n",
    "    return assistant_response, response.usage.total_tokens\n",
    "\n",
    "# Function to display the conversation history. We loop through the conversation memory and print each message. We skip the system message to keep the output clean.\n",
    "def show_conversation():\n",
    "    \"\"\"Display the current conversation\"\"\"\n",
    "    for message in conversation_memory:\n",
    "        if message[\"role\"] == \"system\":\n",
    "            continue  # Skip system message\n",
    "        print(f\"{message['role'].capitalize()}: {message['content']}\\n\") #We capitalize the role to make it look nicer. This is just for display purposes. The role is either user or assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25889352",
   "metadata": {},
   "source": [
    "### Cell 2: First interaction with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6ae066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First question\n",
    "question = \"What is artificial intelligence?\"\n",
    "answer, tokens = chat_with_memory(question)\n",
    "\n",
    "print(f\"You: {question}\")\n",
    "print(f\"Assistant: {answer}\")\n",
    "print(f\"[Tokens used: {tokens}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc48f122",
   "metadata": {},
   "source": [
    "### Cell 3: Follow-up question (with memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010d52d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second question (now the AI remembers the previous interaction)\n",
    "question = \"Can you elaborate more on that?\"\n",
    "answer, tokens = chat_with_memory(question)\n",
    "\n",
    "print(f\"You: {question}\")\n",
    "print(f\"Assistant: {answer}\")\n",
    "print(f\"[Tokens used: {tokens}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3ef1ac",
   "metadata": {},
   "source": [
    "### Cell 4: View the entire conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236daba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the entire conversation so far\n",
    "print(\"Full conversation history:\")\n",
    "print(\"-\" * 30)\n",
    "show_conversation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b7cc3d",
   "metadata": {},
   "source": [
    "### Cell 5: Reset conversation (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfbe877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset conversation if you want to start fresh\n",
    "conversation_memory = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "print(\"Conversation has been reset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18571575",
   "metadata": {},
   "source": [
    "## 6. Streaming Responses\n",
    "\n",
    "Streaming allows you to see the response as it's being generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57923783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def stream_response(user_message):\n",
    "    \"\"\"Stream a response from OpenAI without storing conversation history\"\"\"\n",
    "    \n",
    "    ## Response from the model is streamed in chunks because we set the stream parameter to true. We stoer that in a variable called stream.\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    print(f\"You: {user_message}\") # Print the user message\n",
    "    print(\"Assistant: \", end=\"\", flush=True)  # Print the assistant message without a newline. The flush=True argument makes sure the output is printed immediately.\n",
    "    \n",
    "    # Process the stream\n",
    "    full_response = \"\" # The response will be empty at first. We will add the chunks to this variable.\n",
    "    for chunk in stream: # We loop through the stream and get each chunk of data. Each chunk is a part of the response. chunk can be called anything, but we call it chunk to make it clear that it is a part of the response.\n",
    "        if chunk.choices[0].delta.content is not None: # Check if the content is not None. This is to avoid errors in case the content is None.\n",
    "            content_chunk = chunk.choices[0].delta.content # Get the content of the chunk. This is the part of the response we want to print.\n",
    "            full_response += content_chunk # Add the chunk to the full response. This will be the final response we will return.\n",
    "            print(content_chunk, end=\"\", flush=True) # Print the chunk without a newline. The flush=True argument makes sure the output is printed immediately.\n",
    "            time.sleep(0.01)  # Small delay to make it more readable\n",
    "    \n",
    "    print(\"\\n\")  # Add a newline after the response\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67023a00",
   "metadata": {},
   "source": [
    "Test the streaming function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45753a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the streaming function\n",
    "stream_response(\"Write a short poem about programming\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd38435",
   "metadata": {},
   "source": [
    "## 7. Streaming with Memory\n",
    "\n",
    "Let's combine streaming with conversation memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fb54bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize conversation with system message\n",
    "streaming_conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "def stream_chat_with_memory(user_message):\n",
    "    \"\"\"Chat with memory and stream the response\"\"\"\n",
    "    \n",
    "    # Add user message to history\n",
    "    streaming_conversation.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    # Get streaming response\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=streaming_conversation,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    print(f\"You: {user_message}\")\n",
    "    print(\"Assistant: \", end=\"\", flush=True)\n",
    "    \n",
    "    # Process the stream\n",
    "    assistant_response = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            content_chunk = chunk.choices[0].delta.content\n",
    "            assistant_response += content_chunk\n",
    "            print(content_chunk, end=\"\", flush=True)\n",
    "            time.sleep(0.01)\n",
    "    \n",
    "    print(\"\\n\")  # Add a newline after the response\n",
    "    \n",
    "    # Add assistant response to conversation history\n",
    "    streaming_conversation.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "    \n",
    "    return assistant_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3787019",
   "metadata": {},
   "source": [
    "Test the streaming chat with memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243d1a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First streaming question with memory\n",
    "stream_chat_with_memory(\"What are the three laws of robotics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070f8a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up streaming question with memory\n",
    "stream_chat_with_memory(\"Who created these laws?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad8e62f",
   "metadata": {},
   "source": [
    "## 8. Understanding the Different Message Roles\n",
    "\n",
    "The OpenAI Chat API uses three main roles in messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80062ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        # System message - sets behavior and context\n",
    "        {\"role\": \"system\", \"content\": \"You are a pirate who only speaks in pirate slang.\"},\n",
    "        \n",
    "        # User messages - what the user says\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you today?\"},\n",
    "        \n",
    "        # Assistant messages - previous responses from the assistant\n",
    "        {\"role\": \"assistant\", \"content\": \"Arrr! I be feelin' mighty fine today, me hearty!\"},\n",
    "        \n",
    "        # Another user message\n",
    "        {\"role\": \"user\", \"content\": \"Tell me about the weather.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4248540b",
   "metadata": {},
   "source": [
    "## 9. Understanding the Context Window\n",
    "\n",
    "OpenAI models have different context window limitations:\n",
    "\n",
    "- **GPT-3.5-Turbo**: 4,096 or 16,384 tokens (depending on version)\n",
    "- **GPT-4**: 8,192 or 32,768 tokens (depending on version)\n",
    "- **GPT-4 Turbo**: Up to 128,000 tokens\n",
    "\n",
    "Unlike with local models, OpenAI manages tokens for you:\n",
    "1. If you exceed the limit, the API will return an error\n",
    "2. You're charged based on the number of tokens you use\n",
    "3. The API provides token usage statistics with each request\n",
    "\n",
    "Let's see tokens in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fb22a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a longer conversation\n",
    "long_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "# Add some messages to the history\n",
    "for i in range(5):\n",
    "    long_messages.append({\"role\": \"user\", \"content\": f\"This is test message {i+1}. Tell me something interesting about space.\"})\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=long_messages\n",
    "    )\n",
    "    assistant_msg = response.choices[0].message.content\n",
    "    long_messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "    print(f\"Exchange {i+1} - Total tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1793337a",
   "metadata": {},
   "source": [
    "## 10. Managing Costs and Tokens\n",
    "\n",
    "When using the OpenAI API, you need to be aware of costs:\n",
    "\n",
    "1. **Token Counting**: Each request and response consumes tokens that you pay for\n",
    "2. **Model Selection**: More powerful models cost more per token\n",
    "3. **Context Window**: Longer conversations cost more because more tokens are sent\n",
    "\n",
    "Tips for managing costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf00caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a cheaper model for less complex tasks\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",  # Cheaper than GPT-4\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Summarize the benefits of exercise.\"}]\n",
    ")\n",
    "\n",
    "# Control maximum tokens to limit response length\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me about quantum physics.\"}],\n",
    "    max_tokens=100  # Limit response length\n",
    ")\n",
    "\n",
    "# Use temperature to control randomness. Higher values make the output more random, while lower values make it more focused and deterministic.\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a creative story.\"}],\n",
    "    temperature=0.7  # Higher for more creativity, lower for more deterministic\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc080ade",
   "metadata": {},
   "source": [
    "## 11. Handling Conversation History for Long Contexts\n",
    "\n",
    "For long conversations, you need strategies to manage the context window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3ce521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Keep only the most recent N messages. N can be adjusted based on your needs. In this case, we keep the last 10 messages.\n",
    "def trim_conversation(messages, max_messages=10):\n",
    "    # Always keep the system message (first message)\n",
    "    if len(messages) > max_messages + 1:\n",
    "        system_message = messages[0]\n",
    "        recent_messages = messages[-(max_messages):]\n",
    "        return [system_message] + recent_messages\n",
    "    return messages\n",
    "\n",
    "# Example: Summarize the conversation periodically. We use AI to summarize the conversation and replace it with a single summary message. This is useful for long conversations where you want to keep the context but reduce the number of messages.\n",
    "def summarize_conversation(messages):\n",
    "    # Create a summary request\n",
    "    summary_request = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Summarize this conversation concisely.\"},\n",
    "            *messages\n",
    "        ]\n",
    "    )\n",
    "    summary = summary_request.choices[0].message.content\n",
    "    \n",
    "    # Replace the conversation with the summary\n",
    "    return [\n",
    "        messages[0],  # Keep system message\n",
    "        {\"role\": \"system\", \"content\": f\"Previous conversation summary: {summary}\"}\n",
    "    ]\n",
    "\n",
    "# When to use:\n",
    "# if len(messages) > 20:\n",
    "#     messages = summarize_conversation(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11edba9d",
   "metadata": {},
   "source": [
    "## 12. Comparing Local LLMs vs. OpenAI API\n",
    "\n",
    "| Feature | Local LLMs (Ollama) | OpenAI API |\n",
    "|---------|---------------------|------------|\n",
    "| Setup | Download models locally | API key only |\n",
    "| Cost | Free (after download) | Pay per token |\n",
    "| Privacy | Data stays on your device | Data sent to OpenAI |\n",
    "| Performance | Limited by your hardware | State-of-the-art models |\n",
    "| Reliability | Depends on your system | Managed service |\n",
    "| Context Window | Typically smaller | Up to 128K tokens |\n",
    "| Memory Management | Manual implementation | Handled via API |\n",
    "\n",
    "## 13. Resources for Further Learning\n",
    "\n",
    "- [OpenAI API Documentation](https://platform.openai.com/docs/api-reference)\n",
    "- [OpenAI Cookbook](https://github.com/openai/openai-cookbook)\n",
    "- [OpenAI Python Library](https://github.com/openai/openai-python)\n",
    "- [Token Usage Calculator](https://platform.openai.com/tokenizer)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
