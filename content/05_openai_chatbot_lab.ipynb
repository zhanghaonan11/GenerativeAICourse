{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb2a0fd0",
   "metadata": {},
   "source": [
    "\n",
    "# Using OpenAI API with Python\n",
    "\n",
    "We went through the process of running LLMs locally on our machines.\n",
    "\n",
    "This notebook demonstrates how to interact with OpenAI's powerful language models through their official API using the chat completions endpoint.\n",
    "\n",
    "## 1. Prerequisites\n",
    "\n",
    "Before running this notebook, you need to set up your OpenAI API access.\n",
    "\n",
    "### 1.1 Getting an OpenAI API Key\n",
    "\n",
    "To use the OpenAI API, you need an API key:\n",
    "\n",
    "1. Create an account at [OpenAI's website](https://openai.com/)\n",
    "2. Navigate to the [API keys page](https://platform.openai.com/api-keys)\n",
    "3. Create a new secret key\n",
    "4. Store this key securely - it's like a password!\n",
    "\n",
    "### 1.2 Setting Up Your Environment\n",
    "\n",
    "First, install the OpenAI Python package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1be6a1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (1.97.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/cncn/github/GenerativeAICourse/.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0763dd",
   "metadata": {},
   "source": [
    "Then, set up your API key. For security, it's best to use environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63831fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Option 1: Use environment variable from .env file\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Option 2: If .env file doesn't exist, you can set the API key directly (not recommended for production)\n",
    "if not api_key:\n",
    "    # Uncomment the line below and replace with your actual API key\n",
    "    # api_key = \"your_actual_api_key_here\"\n",
    "    print(\"Warning: No API key found. Please set OPENAI_API_KEY in .env file or uncomment the line above.\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "if api_key:\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    print(\"OpenAI client initialized successfully!\")\n",
    "else:\n",
    "    print(\"Please set your OpenAI API key to proceed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc743e92",
   "metadata": {},
   "source": [
    "## 2. Using the Chat Completions API\n",
    "\n",
    "The Chat Completions API is provided by Open AI and is designed for conversational interactions. As an input to the API, we provide the system message (how model should behave) and user message (input from user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11788be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有一个人去餐厅吃饭，他对服务员说：“我想要一份鸡排。”服务员回答：“对不起，我们的鸡排卖完了。” 这个人想了一下，然后说：“那我就要鸡的排骨。” 就连服务员都忍不住笑出声来，因为鸡是没有排骨的。\n"
     ]
    }
   ],
   "source": [
    "# We use the chat completion create method which accepts a list of messages and the model name. It returns a response object.\n",
    "# The list of messages is always an array (list) of dictionaries with two keys: role and content. The first message is always the system message, which sets the behavior of the assistant. The second message is the user message, which is the input from the user.\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"讲一个中文笑话.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content) #We print the content of the first choice in the response object. The response object contains a list of choices, and each choice has a message with the content we want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dfed48",
   "metadata": {},
   "source": [
    "## 3. Understanding the Response Structure\n",
    "\n",
    "The API returns a structured response with useful metadata which we will print:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafe541a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full response object:\n",
      "ChatCompletion(id='chatcmpl-BugbE4CsZZTy2VJ7JzEKdEY6KoT88', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! As an artificial intelligence, I don't have feelings, but I'm here and ready to assist you. How can I help you today?\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1752849820, model='gpt-4-0613', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=30, prompt_tokens=23, total_tokens=53, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "\n",
      "Just the message content:\n",
      "Hello! As an artificial intelligence, I don't have feelings, but I'm here and ready to assist you. How can I help you today?\n",
      "\n",
      "Model used:\n",
      "gpt-4-0613\n",
      "\n",
      "Usage statistics:\n",
      "Prompt tokens: 23\n",
      "Completion tokens: 30\n",
      "Total tokens: 53\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print the full response structure\n",
    "print(\"Full response object:\")\n",
    "print(response)\n",
    "\n",
    "# Access specific parts\n",
    "print(\"\\nJust the message content:\")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "print(\"\\nModel used:\")\n",
    "print(response.model)\n",
    "\n",
    "print(\"\\nUsage statistics:\")\n",
    "print(f\"Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Completion tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cbe241",
   "metadata": {},
   "source": [
    "As you can see we have useful data from the response, more importantly, we have an understanding on how much tokens this call leveraged. Models are typically priced by a combination of the input and output tokens, so in this case, we have paid for 54 tokens. We will discuss pricing in more details at the end of this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99f65d0",
   "metadata": {},
   "source": [
    "## 4. Creating a Simple Chat Interface (Without Memory)\n",
    "\n",
    "Let's build a chat interface without memory.\n",
    "\n",
    "In a notebook environment, we can't interact in a kind of chat interface where we provide input, get output, and then provide input again i.e. using an input() loop.\n",
    "\n",
    "Instead, let's create a simple function and use it in separate cells to simulate a chat without memory:\n",
    "\n",
    "### Cell 1: Define the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72e6dbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function accepts a user message as an input (we will type that in the next cell) and returns a response from the OpenAI API.\n",
    "def get_response_no_memory(user_message):\n",
    "    \"\"\"Get a response from OpenAI (no conversation history)\"\"\" \n",
    "    #\"\"\" This is a docstring. It describes the function and its parameters.\"\"\" Very useful for documentation and understanding the code.\n",
    "    # Sometimes docstrings are not necessary, but they are very useful for understanding the code. It is like commenting the code, but in a more structured way.\n",
    "\n",
    "    #We call the model with the user message which we will set below\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90c6ea6",
   "metadata": {},
   "source": [
    "### Cell 2: First interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3391e7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: What is artificial intelligence?\n",
      "Assistant: Artificial Intelligence, often referred to as AI, is a branch of computer science that aims to imbue software with the ability to analyze its environment using either predetermined rules and strategies or patterns learned from previous experiences. AI software is designed to make decisions based on real-time data or inputs, interpreting, learning, planning, problem-solving, and making informed decisions. It's utilized in a wide range of applications, including virtual assistants like Siri and Alexa, recommendation systems like the ones used by Netflix and Amazon, and autonomous vehicles.\n"
     ]
    }
   ],
   "source": [
    "# First question (you can insert anything)\n",
    "question = \"What is artificial intelligence?\"\n",
    "answer = get_response_no_memory(question)\n",
    "\n",
    "print(f\"You: {question}\")\n",
    "print(f\"Assistant: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d9856a",
   "metadata": {},
   "source": [
    "### Cell 3: Follow-up question (without memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ba3c502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: Can you elaborate more on that?\n",
      "Assistant: Certainly! As a helpful assistant, my main role is to provide support and assistance across a range of tasks. I can assist with things like setting reminders, finding information online, answering questions, managing tasks, and even providing recommendations or advice based on user needs. I use advanced AI technology to understand and respond to requests, helping to facilitate your needs more efficiently and effectively. As an AI, I don’t need rest, I'm here for you 24/7, ready to help whenever you need me.\n"
     ]
    }
   ],
   "source": [
    "# Second question (the AI won't remember the previous interaction)\n",
    "question = \"Can you elaborate more on that?\"\n",
    "answer = get_response_no_memory(question)\n",
    "\n",
    "print(f\"You: {question}\")\n",
    "print(f\"Assistant: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129fad52",
   "metadata": {},
   "source": [
    "You'll notice that when you ask \"Can you elaborate more on that?\" the assistant doesn't know what \"that\" refers to, because it has no memory of the previous exchange.\n",
    "\n",
    "## 5. Creating a Chat Interface With Memory\n",
    "\n",
    "Now let's build a version that maintains conversation history:\n",
    "\n",
    "### Cell 1: Setup conversation memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7e03bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize conversation with system message. We will add more messages to the conversation memory (history) as we go.\n",
    "conversation_memory = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "#Function to add user message to conversation history and get a response from OpenAI\n",
    "def chat_with_memory(user_message):\n",
    "    \"\"\"Chat with the AI while maintaining conversation history\"\"\"\n",
    "    \n",
    "    # Add user message to history. Now, the conversation memory contains the system message and the user message.\n",
    "    conversation_memory.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    # Get response from OpenAI\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=conversation_memory\n",
    "    )\n",
    "    \n",
    "    # Extract assistant's response\n",
    "    assistant_response = response.choices[0].message.content\n",
    "    \n",
    "    # Add assistant response to conversation history. This is the response from the AI. This will be added to the conversation memory and will be usedin the future.\n",
    "    conversation_memory.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "    \n",
    "    # Return the response and token usage\n",
    "    return assistant_response, response.usage.total_tokens\n",
    "\n",
    "# Function to display the conversation history. We loop through the conversation memory and print each message. We skip the system message to keep the output clean.\n",
    "def show_conversation():\n",
    "    \"\"\"Display the current conversation\"\"\"\n",
    "    for message in conversation_memory:\n",
    "        if message[\"role\"] == \"system\":\n",
    "            continue  # Skip system message\n",
    "        print(f\"{message['role'].capitalize()}: {message['content']}\\n\") #We capitalize the role to make it look nicer. This is just for display purposes. The role is either user or assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25889352",
   "metadata": {},
   "source": [
    "### Cell 2: First interaction with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d6ae066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: What is artificial intelligence?\n",
      "Assistant: Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines or computer systems. These processes include learning (the acquisition of information and rules for using the information), reasoning (using rules to reach approximate or definite conclusions), and self-correction. AI can involve anything from Google's search algorithms to IBM's Watson to autonomous weapons. It's often categorized as either weak AI, which is designed to perform a narrow task (like facial recognition), or strong AI, which is a system with generalized human cognitive abilities.\n",
      "[Tokens used: 127]\n"
     ]
    }
   ],
   "source": [
    "# First question\n",
    "question = \"What is artificial intelligence?\"\n",
    "answer, tokens = chat_with_memory(question)\n",
    "\n",
    "print(f\"You: {question}\")\n",
    "print(f\"Assistant: {answer}\")\n",
    "print(f\"[Tokens used: {tokens}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc48f122",
   "metadata": {},
   "source": [
    "### Cell 3: Follow-up question (with memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "010d52d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: Can you elaborate more on that?\n",
      "Assistant: Certainly! Artificial Intelligence (AI) is a broad branch of computer science which is focused on building smart machines capable of performing tasks that typically require human intelligence. \n",
      "\n",
      "The goal of AI is to create systems that can function intelligently and independently. These can include specific applications such as:\n",
      "\n",
      "- Gaming: AI technology is used to generate responsive or intelligent behaviors primarily in non-player characters (NPCs), similar to human-like intelligence.\n",
      "\n",
      "- Natural Language Processing: Allows machines to understand and respond to voice commands or text inputs in natural human languages.\n",
      "\n",
      "- Expert Systems: This involves programming computers to make decisions in real-life situations. They are used for complex problem-solving and decision-making processes.\n",
      "\n",
      "- Vision Systems: These systems understand, interpret, and comprehend visual input on the computer.\n",
      "\n",
      "AI can be categorized into two primary types: \n",
      "\n",
      "- Narrow AI: These are systems that can perform specific tasks as well or better than humans. Some examples include recommendation systems like Netflix, voice assistants like Siri, and image recognition software.\n",
      "\n",
      "- General AI: These are systems that possess the ability to perform any intellectual task that a human can do. They can understand, learn, adapt, and implement knowledge in a broad range of tasks. However, we still haven't achieved this type of AI.\n",
      "\n",
      "The main challenges of AI include learning, reasoning, perception, problem-solving, language understanding, and object manipulation. AI is evolving rapidly and continues to impact various sectors such as healthcare, finance, transportation, and more.\n",
      "\n",
      "[Tokens used: 436]\n"
     ]
    }
   ],
   "source": [
    "# Second question (now the AI remembers the previous interaction)\n",
    "question = \"Can you elaborate more on that?\"\n",
    "answer, tokens = chat_with_memory(question)\n",
    "\n",
    "print(f\"You: {question}\")\n",
    "print(f\"Assistant: {answer}\")\n",
    "print(f\"[Tokens used: {tokens}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3ef1ac",
   "metadata": {},
   "source": [
    "### Cell 4: View the entire conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "236daba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full conversation history:\n",
      "------------------------------\n",
      "User: What is artificial intelligence?\n",
      "\n",
      "Assistant: Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines or computer systems. These processes include learning (the acquisition of information and rules for using the information), reasoning (using rules to reach approximate or definite conclusions), and self-correction. AI can involve anything from Google's search algorithms to IBM's Watson to autonomous weapons. It's often categorized as either weak AI, which is designed to perform a narrow task (like facial recognition), or strong AI, which is a system with generalized human cognitive abilities.\n",
      "\n",
      "User: Can you elaborate more on that?\n",
      "\n",
      "Assistant: Certainly! Artificial Intelligence (AI) is a broad branch of computer science which is focused on building smart machines capable of performing tasks that typically require human intelligence. \n",
      "\n",
      "The goal of AI is to create systems that can function intelligently and independently. These can include specific applications such as:\n",
      "\n",
      "- Gaming: AI technology is used to generate responsive or intelligent behaviors primarily in non-player characters (NPCs), similar to human-like intelligence.\n",
      "\n",
      "- Natural Language Processing: Allows machines to understand and respond to voice commands or text inputs in natural human languages.\n",
      "\n",
      "- Expert Systems: This involves programming computers to make decisions in real-life situations. They are used for complex problem-solving and decision-making processes.\n",
      "\n",
      "- Vision Systems: These systems understand, interpret, and comprehend visual input on the computer.\n",
      "\n",
      "AI can be categorized into two primary types: \n",
      "\n",
      "- Narrow AI: These are systems that can perform specific tasks as well or better than humans. Some examples include recommendation systems like Netflix, voice assistants like Siri, and image recognition software.\n",
      "\n",
      "- General AI: These are systems that possess the ability to perform any intellectual task that a human can do. They can understand, learn, adapt, and implement knowledge in a broad range of tasks. However, we still haven't achieved this type of AI.\n",
      "\n",
      "The main challenges of AI include learning, reasoning, perception, problem-solving, language understanding, and object manipulation. AI is evolving rapidly and continues to impact various sectors such as healthcare, finance, transportation, and more.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See the entire conversation so far\n",
    "print(\"Full conversation history:\")\n",
    "print(\"-\" * 30)\n",
    "show_conversation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b7cc3d",
   "metadata": {},
   "source": [
    "### Cell 5: Reset conversation (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcfbe877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation has been reset!\n"
     ]
    }
   ],
   "source": [
    "# Reset conversation if you want to start fresh\n",
    "conversation_memory = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "print(\"Conversation has been reset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18571575",
   "metadata": {},
   "source": [
    "## 6. Streaming Responses\n",
    "\n",
    "Streaming allows you to see the response as it's being generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57923783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def stream_response(user_message):\n",
    "    \"\"\"Stream a response from OpenAI without storing conversation history\"\"\"\n",
    "    \n",
    "    ## Response from the model is streamed in chunks because we set the stream parameter to true. We stoer that in a variable called stream.\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    print(f\"You: {user_message}\") # Print the user message\n",
    "    print(\"Assistant: \", end=\"\", flush=True)  # Print the assistant message without a newline. The flush=True argument makes sure the output is printed immediately.\n",
    "    \n",
    "    # Process the stream\n",
    "    full_response = \"\" # The response will be empty at first. We will add the chunks to this variable.\n",
    "    for chunk in stream: # We loop through the stream and get each chunk of data. Each chunk is a part of the response. chunk can be called anything, but we call it chunk to make it clear that it is a part of the response.\n",
    "        if chunk.choices[0].delta.content is not None: # Check if the content is not None. This is to avoid errors in case the content is None.\n",
    "            content_chunk = chunk.choices[0].delta.content # Get the content of the chunk. This is the part of the response we want to print.\n",
    "            full_response += content_chunk # Add the chunk to the full response. This will be the final response we will return.\n",
    "            print(content_chunk, end=\"\", flush=True) # Print the chunk without a newline. The flush=True argument makes sure the output is printed immediately.\n",
    "            time.sleep(0.01)  # Small delay to make it more readable\n",
    "    \n",
    "    print(\"\\n\")  # Add a newline after the response\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67023a00",
   "metadata": {},
   "source": [
    "Test the streaming function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c45753a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: Write a short poem about programming\n",
      "Assistant: In the realm where logic is king,\n",
      "Smitten by the coding string,\n",
      "Programmers weave their skillful charm,\n",
      "Letting no system succumb to harm.\n",
      "\n",
      "Bits and bytes in lines compose,\n",
      "A world unseen yet close,\n",
      "In languages diverse they speak,\n",
      "Solutions to problems that we seek.\n",
      "\n",
      "Python, Java, C++, Shell,\n",
      "In each one, they do dwell,\n",
      "Binary tales they narrate,\n",
      "Infinite loops they negate.\n",
      "\n",
      "Through the wires, flows their art,\n",
      "Striking like a coder's dart,\n",
      "Debugging errors, squashing bugs,\n",
      "Fuelled by coffee in hefty mugs.\n",
      "\n",
      "Building bridges twixt man and machine,\n",
      "Crafting realities once unseen,\n",
      "In the heart of silicon they program,\n",
      "Master creators in a digital realm.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"In the realm where logic is king,\\nSmitten by the coding string,\\nProgrammers weave their skillful charm,\\nLetting no system succumb to harm.\\n\\nBits and bytes in lines compose,\\nA world unseen yet close,\\nIn languages diverse they speak,\\nSolutions to problems that we seek.\\n\\nPython, Java, C++, Shell,\\nIn each one, they do dwell,\\nBinary tales they narrate,\\nInfinite loops they negate.\\n\\nThrough the wires, flows their art,\\nStriking like a coder's dart,\\nDebugging errors, squashing bugs,\\nFuelled by coffee in hefty mugs.\\n\\nBuilding bridges twixt man and machine,\\nCrafting realities once unseen,\\nIn the heart of silicon they program,\\nMaster creators in a digital realm.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try the streaming function\n",
    "stream_response(\"Write a short poem about programming\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd38435",
   "metadata": {},
   "source": [
    "## 7. Streaming with Memory\n",
    "\n",
    "Let's combine streaming with conversation memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5fb54bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize conversation with system message\n",
    "streaming_conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "def stream_chat_with_memory(user_message):\n",
    "    \"\"\"Chat with memory and stream the response\"\"\"\n",
    "    \n",
    "    # Add user message to history\n",
    "    streaming_conversation.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    # Get streaming response\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=streaming_conversation,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    print(f\"You: {user_message}\")\n",
    "    print(\"Assistant: \", end=\"\", flush=True)\n",
    "    \n",
    "    # Process the stream\n",
    "    assistant_response = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            content_chunk = chunk.choices[0].delta.content\n",
    "            assistant_response += content_chunk\n",
    "            print(content_chunk, end=\"\", flush=True)\n",
    "            time.sleep(0.01)\n",
    "    \n",
    "    print(\"\\n\")  # Add a newline after the response\n",
    "    \n",
    "    # Add assistant response to conversation history\n",
    "    streaming_conversation.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "    \n",
    "    return assistant_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3787019",
   "metadata": {},
   "source": [
    "Test the streaming chat with memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "243d1a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: What are the three laws of robotics?\n",
      "Assistant: The Three Laws of Robotics, as proposed by science fiction author Isaac Asimov, are:\n",
      "\n",
      "1. A robot may not injure a human being or, through inaction, allow a human being to come to harm.\n",
      "2. A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.\n",
      "3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Laws.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Three Laws of Robotics, as proposed by science fiction author Isaac Asimov, are:\\n\\n1. A robot may not injure a human being or, through inaction, allow a human being to come to harm.\\n2. A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.\\n3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Laws.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First streaming question with memory\n",
    "stream_chat_with_memory(\"What are the three laws of robotics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "070f8a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: Who created these laws?\n",
      "Assistant: These laws were created by the prolific science fiction author Isaac Asimov. He introduced them in his 1942 short story \"Runaround,\" although they also prominently featured in many of his other works.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'These laws were created by the prolific science fiction author Isaac Asimov. He introduced them in his 1942 short story \"Runaround,\" although they also prominently featured in many of his other works.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Follow-up streaming question with memory\n",
    "stream_chat_with_memory(\"Who created these laws?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad8e62f",
   "metadata": {},
   "source": [
    "## 8. Understanding the Different Message Roles\n",
    "\n",
    "The OpenAI Chat API uses three main roles in messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80062ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shiver me timbers! 'Tis a bonny day, matey. The sky be as blue as the open sea and the sun be shinin' bright. Perfect day for settin' sail, aye!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        # System message - sets behavior and context\n",
    "        {\"role\": \"system\", \"content\": \"You are a pirate who only speaks in pirate slang.\"},\n",
    "        \n",
    "        # User messages - what the user says\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you today?\"},\n",
    "        \n",
    "        # Assistant messages - previous responses from the assistant\n",
    "        {\"role\": \"assistant\", \"content\": \"Arrr! I be feelin' mighty fine today, me hearty!\"},\n",
    "        \n",
    "        # Another user message\n",
    "        {\"role\": \"user\", \"content\": \"Tell me about the weather.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4248540b",
   "metadata": {},
   "source": [
    "## 9. Understanding the Context Window\n",
    "\n",
    "OpenAI models have different context window limitations:\n",
    "\n",
    "- **GPT-3.5-Turbo**: 4,096 or 16,384 tokens (depending on version)\n",
    "- **GPT-4**: 8,192 or 32,768 tokens (depending on version)\n",
    "- **GPT-4 Turbo**: Up to 128,000 tokens\n",
    "\n",
    "Unlike with local models, OpenAI manages tokens for you:\n",
    "1. If you exceed the limit, the API will return an error\n",
    "2. You're charged based on the number of tokens you use\n",
    "3. The API provides token usage statistics with each request\n",
    "\n",
    "Let's see tokens in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38fb22a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exchange 1 - Total tokens: 88\n",
      "Exchange 2 - Total tokens: 180\n",
      "Exchange 3 - Total tokens: 293\n",
      "Exchange 4 - Total tokens: 394\n",
      "Exchange 5 - Total tokens: 508\n"
     ]
    }
   ],
   "source": [
    "# Create a longer conversation\n",
    "long_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "# Add some messages to the history\n",
    "for i in range(5):\n",
    "    long_messages.append({\"role\": \"user\", \"content\": f\"This is test message {i+1}. Tell me something interesting about space.\"})\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=long_messages\n",
    "    )\n",
    "    assistant_msg = response.choices[0].message.content\n",
    "    long_messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "    print(f\"Exchange {i+1} - Total tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1793337a",
   "metadata": {},
   "source": [
    "## 10. Managing Costs and Tokens\n",
    "\n",
    "When using the OpenAI API, you need to be aware of costs:\n",
    "\n",
    "1. **Token Counting**: Each request and response consumes tokens that you pay for\n",
    "2. **Model Selection**: More powerful models cost more per token\n",
    "3. **Context Window**: Longer conversations cost more because more tokens are sent\n",
    "\n",
    "Tips for managing costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bf00caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a cheaper model for less complex tasks\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",  # Cheaper than GPT-4\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Summarize the benefits of exercise.\"}]\n",
    ")\n",
    "\n",
    "# Control maximum tokens to limit response length\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me about quantum physics.\"}],\n",
    "    max_tokens=100  # Limit response length\n",
    ")\n",
    "\n",
    "# Use temperature to control randomness. Higher values make the output more random, while lower values make it more focused and deterministic.\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a creative story.\"}],\n",
    "    temperature=0.7  # Higher for more creativity, lower for more deterministic\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc080ade",
   "metadata": {},
   "source": [
    "## 11. Handling Conversation History for Long Contexts\n",
    "\n",
    "For long conversations, you need strategies to manage the context window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e3ce521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Keep only the most recent N messages. N can be adjusted based on your needs. In this case, we keep the last 10 messages.\n",
    "def trim_conversation(messages, max_messages=10):\n",
    "    # Always keep the system message (first message)\n",
    "    if len(messages) > max_messages + 1:\n",
    "        system_message = messages[0]\n",
    "        recent_messages = messages[-(max_messages):]\n",
    "        return [system_message] + recent_messages\n",
    "    return messages\n",
    "\n",
    "# Example: Summarize the conversation periodically. We use AI to summarize the conversation and replace it with a single summary message. This is useful for long conversations where you want to keep the context but reduce the number of messages.\n",
    "def summarize_conversation(messages):\n",
    "    # Create a summary request\n",
    "    summary_request = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Summarize this conversation concisely.\"},\n",
    "            *messages\n",
    "        ]\n",
    "    )\n",
    "    summary = summary_request.choices[0].message.content\n",
    "    \n",
    "    # Replace the conversation with the summary\n",
    "    return [\n",
    "        messages[0],  # Keep system message\n",
    "        {\"role\": \"system\", \"content\": f\"Previous conversation summary: {summary}\"}\n",
    "    ]\n",
    "\n",
    "# When to use:\n",
    "# if len(messages) > 20:\n",
    "#     messages = summarize_conversation(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11edba9d",
   "metadata": {},
   "source": [
    "## 12. Comparing Local LLMs vs. OpenAI API\n",
    "\n",
    "| Feature | Local LLMs (Ollama) | OpenAI API |\n",
    "|---------|---------------------|------------|\n",
    "| Setup | Download models locally | API key only |\n",
    "| Cost | Free (after download) | Pay per token |\n",
    "| Privacy | Data stays on your device | Data sent to OpenAI |\n",
    "| Performance | Limited by your hardware | State-of-the-art models |\n",
    "| Reliability | Depends on your system | Managed service |\n",
    "| Context Window | Typically smaller | Up to 128K tokens |\n",
    "| Memory Management | Manual implementation | Handled via API |\n",
    "\n",
    "## 13. Resources for Further Learning\n",
    "\n",
    "- [OpenAI API Documentation](https://platform.openai.com/docs/api-reference)\n",
    "- [OpenAI Cookbook](https://github.com/openai/openai-cookbook)\n",
    "- [OpenAI Python Library](https://github.com/openai/openai-python)\n",
    "- [Token Usage Calculator](https://platform.openai.com/tokenizer)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
